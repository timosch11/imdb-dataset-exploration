{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Preprocessed Train, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed train data from a csv file\n",
    "train = pd.read_csv('preprocessed/train.csv')\n",
    "\n",
    "# Load the preprocessed validation data from a csv file\n",
    "val = pd.read_csv('preprocessed/val.csv')\n",
    "\n",
    "# Load the preprocessed test data from a csv file\n",
    "test = pd.read_csv('preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work librari expect like movi came 5 year ago ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eagl wing pleasant surpris movi keep viewer in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york love collect work eleven short film s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saw movi yesterday night one best made tv film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>playwright sidney bruhl wonder overthetop mich...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34702</th>\n",
       "      <td>love movi tv program record come nov 2nd reall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34703</th>\n",
       "      <td>big jim carey fan took seat cinema optim fun d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34704</th>\n",
       "      <td>even 6000 buck cast parttim actor christoph no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34705</th>\n",
       "      <td>one best movi ive ever seen good act hank newm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34706</th>\n",
       "      <td>grow voyag space favorit movi rememb time ktla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34707 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      work librari expect like movi came 5 year ago ...          1\n",
       "1      eagl wing pleasant surpris movi keep viewer in...          1\n",
       "2      new york love collect work eleven short film s...          1\n",
       "3      saw movi yesterday night one best made tv film...          1\n",
       "4      playwright sidney bruhl wonder overthetop mich...          1\n",
       "...                                                  ...        ...\n",
       "34702  love movi tv program record come nov 2nd reall...          1\n",
       "34703  big jim carey fan took seat cinema optim fun d...          0\n",
       "34704  even 6000 buck cast parttim actor christoph no...          1\n",
       "34705  one best movi ive ever seen good act hank newm...          1\n",
       "34706  grow voyag space favorit movi rememb time ktla...          1\n",
       "\n",
       "[34707 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a Dataset into a PyTorch Dataset for Sentiment Analysis using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define a custom PyTorch dataset for sentiment analysis\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        # Store the reviews and labels as instance variables\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the review and label at the given index\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert the review to inputs for the DistilBERT model\n",
    "        input_ids = torch.tensor(tokenizer.encode(review, add_special_tokens=True))\n",
    "        max_length = 128 # replace with your desired maximum sequence length\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        input_ids = torch.nn.functional.pad(input_ids, (0, padding_length), value=0)\n",
    "        attention_mask = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "        # Return the inputs and label as a dictionary\n",
    "        return {\n",
    "            \"input_ids\": input_ids.unsqueeze(0),\n",
    "            \"attention_mask\": attention_mask.unsqueeze(0),\n",
    "            \"labels\": torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a SentimentDataset for the train data\n",
    "train_dataset = SentimentDataset(train[\"review\"], train[\"sentiment\"])\n",
    "\n",
    "# Create a SentimentDataset for the validation data\n",
    "val_dataset = SentimentDataset(val[\"review\"], val[\"sentiment\"])\n",
    "\n",
    "# Create a SentimentDataset for the test data\n",
    "test_dataset = SentimentDataset(test[\"review\"], test[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pre-trained DistilBERT Model and Defining the Loss Function, Optimizer, and Learning Rate Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=len(train_dataset) * epochs, epochs=epochs)\n",
    "\n",
    "# Define an additional criterion for calculating loss\n",
    "criterion = torch.nn.BCELoss()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Train and Validation Data Loaders and Specifying the Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the validation data loader\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Specify the device to use for training (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the specified device\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the DistilBERT Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timos\\Documents\\Programming for Data Science\\sentiment_analysis\\sentiment_analysis2.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis2.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, labels_one_hot)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis2.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Backward pass: compute gradients of loss with respect to model parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis2.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis2.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Update model parameters using computed gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis2.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Print the current epoch number\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "    \n",
    "    # Initialize variables to track the train and validation loss and accuracy\n",
    "    train_loss, val_loss = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    \n",
    "    # Record the start time of the epoch\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over the batches in the train data loader\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Move data to proper dtype and device\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Convert labels to one-hot encoding\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predictions from inputs\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0] # get raw logits from model\n",
    "\n",
    "        # Compute loss using raw logits and one-hot encoded labels\n",
    "        loss = loss_fn(logits, labels_one_hot)\n",
    "        \n",
    "        # Backward pass: compute gradients of loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters using computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate using scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        train_acc += (pred == labels).sum().item()\n",
    "\n",
    "        # Print progress information\n",
    "        if (i+1) % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (i+1) * (len(train_loader) - (i+1))\n",
    "            current_acc = train_acc / ((i+1) * 16)\n",
    "            print(\"Batch: {}/{} - Elapsed Time: {:.2f}s - Remaining Time: {:.2f}s - Train Acc: {:.4f}\".format(i+1, len(train_loader), elapsed_time, remaining_time, current_acc))\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            input_ids = batch[\"input_ids\"].squeeze(1)\n",
    "            attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
    "            labels = batch[\"labels\"]\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=2).float() # convert labels to one-hot encoding\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[0] # get raw logits from model\n",
    "\n",
    "            loss = loss_fn(logits, labels_one_hot) # compute loss using raw logits and one-hot encoded labels\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            val_acc += (pred == labels).sum().item()\n",
    "\n",
    "    # Calculate the average losses and accuracies\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    train_acc = train_acc / len(train_dataset)\n",
    "    val_acc = val_acc / len(val_dataset)\n",
    "\n",
    "    # Print the losses and accuracies\n",
    "    print(\"Train Loss: {:.4f}, Val Loss: {:.4f}\".format(train_loss, val_loss))\n",
    "    print(\"Train Acc: {:.4f}, Val Acc: {:.4f}\".format(train_acc, val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290f75e68ed0da714247ccd610ce4a15a5bfa48e2a04a943182beeac16de2706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
