{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimmungsanalyse für IMDB-Kommentare (Transformer)\n",
    "\n",
    "In diesem Projekt führen wir eine Stimmungsanalyse der IMDB-Kommentare durch, um die allgemeine Stimmung der Kommentare zu ermitteln."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir die vorverarbeiteten Daten mit der Bibliothek \"pandas\". Die Daten werden in drei Sätze aufgeteilt: Training, Validierung und Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('preprocessed/train.csv')\n",
    "val = pd.read_csv('preprocessed/val.csv')\n",
    "test = pd.read_csv('preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work librari expect like movi came 5 year ago ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eagl wing pleasant surpris movi keep viewer in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york love collect work eleven short film s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saw movi yesterday night one best made tv film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>playwright sidney bruhl wonder overthetop mich...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34702</th>\n",
       "      <td>love movi tv program record come nov 2nd reall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34703</th>\n",
       "      <td>big jim carey fan took seat cinema optim fun d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34704</th>\n",
       "      <td>even 6000 buck cast parttim actor christoph no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34705</th>\n",
       "      <td>one best movi ive ever seen good act hank newm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34706</th>\n",
       "      <td>grow voyag space favorit movi rememb time ktla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34707 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      work librari expect like movi came 5 year ago ...          1\n",
       "1      eagl wing pleasant surpris movi keep viewer in...          1\n",
       "2      new york love collect work eleven short film s...          1\n",
       "3      saw movi yesterday night one best made tv film...          1\n",
       "4      playwright sidney bruhl wonder overthetop mich...          1\n",
       "...                                                  ...        ...\n",
       "34702  love movi tv program record come nov 2nd reall...          1\n",
       "34703  big jim carey fan took seat cinema optim fun d...          0\n",
       "34704  even 6000 buck cast parttim actor christoph no...          1\n",
       "34705  one best movi ive ever seen good act hank newm...          1\n",
       "34706  grow voyag space favorit movi rememb time ktla...          1\n",
       "\n",
       "[34707 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen das Model `DistilBERT` von der `transformers` Bibliothek für unsere Sentiment-Analyse. Wir benutzen außerdem den `DistilBERT` tokenizer um unsere Text Daten zu tokenisieren.\n",
    "\n",
    "Im Code importieren wir zunächst die notwendigen Bibliotheken für die Arbeit mit PyTorch und die Bibliothek \"Transformers\". Dann laden wir den Tokenizer \"DistilBERT\", den wir zur Tokenisierung unserer Textdaten verwenden werden.\n",
    "\n",
    "Als nächstes definieren wir eine benutzerdefinierte \"SentimentDataset\"-Klasse. Diese Klasse nimmt eine Liste von reviews und eine Liste von labels als Eingabe auf. Die Methode `__len__` gibt die Länge des Datensatzes zurück, während die Methode `__getitem__` ein Wörterbuch zurückgibt, das die tokenisierten Reviews, die Attention-Mask und das Label für einen bestimmten Index enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Convert your dataset into a PyTorch dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews # store the list of reviews\n",
    "        self.labels = labels # store the list of labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews) # return the length of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx] # get the review at the given index\n",
    "        label = self.labels[idx] # get the label at the given index\n",
    "\n",
    "        # Convert the review to inputs for the DistilBERT model\n",
    "        input_ids = torch.tensor(tokenizer.encode(review, add_special_tokens=True)) # tokenize the review and get its input_ids\n",
    "        max_length = 128 # set the maximum sequence length to 128\n",
    "        padding_length = max_length - len(input_ids) # calculate the amount of padding needed\n",
    "        input_ids = torch.nn.functional.pad(input_ids, (0, padding_length), value=0) # pad the input sequence with zeros\n",
    "        attention_mask = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0)) # create an attention mask that distinguishes between padded and unpadded parts of the sequence\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.unsqueeze(0), # add a batch dimension to the input_ids tensor\n",
    "            \"attention_mask\": attention_mask.unsqueeze(0), # add a batch dimension to the attention_mask tensor\n",
    "            \"labels\": torch.tensor(label) # convert the label to a tensor and include it in the output dictionary\n",
    "        }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir unsere benutzerdefinierte SentimentDataset-Klasse definiert haben, können wir sie verwenden, um PyTorch-Datensätze für unsere Trainings-, Validierungs- und Testdaten zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PyTorch datasets for training, validation, and testing\n",
    "train_dataset = SentimentDataset(train[\"review\"], train[\"sentiment\"]) # create a dataset for training data using the SentimentDataset class\n",
    "val_dataset = SentimentDataset(val[\"review\"], val[\"sentiment\"]) # create a dataset for validation data using the SentimentDataset class\n",
    "test_dataset = SentimentDataset(test[\"review\"], test[\"sentiment\"]) # create a dataset for test data using the SentimentDataset class\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir unser Modell trainieren können, müssen wir einige Hyperparameter definieren und das vortrainierte Modell \"DistilBERT\" laden.\n",
    "\n",
    "Wir setzen die Anzahl der Trainingsepochen auf 2 und laden das vortrainierte DistilBERT-Modell für die Sequenzklassifikation.\n",
    "\n",
    "Als nächstes definieren wir die Loss-FunKtion, den Optimizer und den lr_scheduler, die wir während des Trainings verwenden werden. Wir verwenden binary cross-entropy loss als Verlustfunktion, da wir ein binäres Klassifikationsproblem lösen. Wir verwenden den Adam-Optimizer mit einer Lr von 2e-5, um die Lernrate im Laufe des Trainings schrittweise zu erhöhen und dann zu verringern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "# Load the pre-trained DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "# binary cross-entropy loss for classification problem\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "# Adam optimizer with a learning rate of 2e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5) \n",
    " # one-cycle learning rate scheduler to gradually increase \n",
    " # and then decrease the learning rate over the course of training\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                 max_lr=2e-5,\n",
    "                                                 total_steps=len(train_dataset) * epochs, \n",
    "                                                 epochs=epochs)\n",
    "# binary cross-entropy loss for binary classification problems \n",
    "criterion = torch.nn.BCELoss() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and validation data loaders\n",
    "batch_size = 16 # number of samples in each batch\n",
    " # data loader for training data, shuffles data at each epoch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    " # data loader for validation data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir unser Model trainieren\n",
    "\n",
    "ImCode durchlaufen wir jede Epoche in einer Schleife und trainieren unser Modell anhand der Trainingsdaten. Bei jeder Iteration der inneren Schleife holen wir einen Datenstapel aus dem train_loader, konvertieren die Daten in das richtige Format und konvertieren die Beschriftungen in eine One-Hot-Codierung. \n",
    "\n",
    "Nach  einer Epoche bewerten wir unser Modell anhand der Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n",
      "Batch: 10/2170 - Elapsed Time: 34.81s - Remaining Time: 7518.23s - Train Acc: 0.5125\n",
      "Batch: 20/2170 - Elapsed Time: 62.48s - Remaining Time: 6716.38s - Train Acc: 0.5156\n",
      "Batch: 30/2170 - Elapsed Time: 93.39s - Remaining Time: 6661.66s - Train Acc: 0.5437\n",
      "Batch: 40/2170 - Elapsed Time: 127.03s - Remaining Time: 6764.51s - Train Acc: 0.5359\n",
      "Batch: 50/2170 - Elapsed Time: 160.77s - Remaining Time: 6816.76s - Train Acc: 0.5238\n",
      "Batch: 60/2170 - Elapsed Time: 193.24s - Remaining Time: 6795.64s - Train Acc: 0.5365\n",
      "Batch: 70/2170 - Elapsed Time: 226.19s - Remaining Time: 6785.69s - Train Acc: 0.5321\n",
      "Batch: 80/2170 - Elapsed Time: 256.90s - Remaining Time: 6711.56s - Train Acc: 0.5242\n",
      "Batch: 90/2170 - Elapsed Time: 287.48s - Remaining Time: 6644.01s - Train Acc: 0.5194\n",
      "Batch: 100/2170 - Elapsed Time: 318.21s - Remaining Time: 6586.98s - Train Acc: 0.5206\n",
      "Batch: 110/2170 - Elapsed Time: 348.92s - Remaining Time: 6534.25s - Train Acc: 0.5210\n",
      "Batch: 120/2170 - Elapsed Time: 379.65s - Remaining Time: 6485.63s - Train Acc: 0.5208\n",
      "Batch: 130/2170 - Elapsed Time: 410.39s - Remaining Time: 6439.92s - Train Acc: 0.5183\n",
      "Batch: 140/2170 - Elapsed Time: 442.68s - Remaining Time: 6418.93s - Train Acc: 0.5205\n",
      "Batch: 150/2170 - Elapsed Time: 478.29s - Remaining Time: 6440.93s - Train Acc: 0.5204\n",
      "Batch: 160/2170 - Elapsed Time: 510.99s - Remaining Time: 6419.31s - Train Acc: 0.5250\n",
      "Batch: 170/2170 - Elapsed Time: 542.56s - Remaining Time: 6383.08s - Train Acc: 0.5243\n",
      "Batch: 180/2170 - Elapsed Time: 573.45s - Remaining Time: 6339.77s - Train Acc: 0.5281\n",
      "Batch: 190/2170 - Elapsed Time: 602.91s - Remaining Time: 6282.96s - Train Acc: 0.5336\n",
      "Batch: 200/2170 - Elapsed Time: 632.69s - Remaining Time: 6231.99s - Train Acc: 0.5341\n",
      "Batch: 210/2170 - Elapsed Time: 661.91s - Remaining Time: 6177.83s - Train Acc: 0.5327\n",
      "Batch: 220/2170 - Elapsed Time: 691.41s - Remaining Time: 6128.41s - Train Acc: 0.5349\n",
      "Batch: 230/2170 - Elapsed Time: 720.55s - Remaining Time: 6077.67s - Train Acc: 0.5372\n",
      "Batch: 240/2170 - Elapsed Time: 750.53s - Remaining Time: 6035.49s - Train Acc: 0.5372\n",
      "Batch: 250/2170 - Elapsed Time: 779.96s - Remaining Time: 5990.13s - Train Acc: 0.5370\n",
      "Batch: 260/2170 - Elapsed Time: 809.64s - Remaining Time: 5947.72s - Train Acc: 0.5380\n",
      "Batch: 270/2170 - Elapsed Time: 838.68s - Remaining Time: 5901.82s - Train Acc: 0.5352\n",
      "Batch: 280/2170 - Elapsed Time: 868.16s - Remaining Time: 5860.07s - Train Acc: 0.5355\n",
      "Batch: 290/2170 - Elapsed Time: 897.35s - Remaining Time: 5817.33s - Train Acc: 0.5369\n",
      "Batch: 300/2170 - Elapsed Time: 926.87s - Remaining Time: 5777.47s - Train Acc: 0.5369\n",
      "Batch: 310/2170 - Elapsed Time: 956.07s - Remaining Time: 5736.44s - Train Acc: 0.5391\n",
      "Batch: 320/2170 - Elapsed Time: 985.60s - Remaining Time: 5698.02s - Train Acc: 0.5395\n",
      "Batch: 330/2170 - Elapsed Time: 1015.46s - Remaining Time: 5661.98s - Train Acc: 0.5413\n",
      "Batch: 340/2170 - Elapsed Time: 1045.99s - Remaining Time: 5629.90s - Train Acc: 0.5421\n",
      "Batch: 350/2170 - Elapsed Time: 1075.63s - Remaining Time: 5593.25s - Train Acc: 0.5429\n",
      "Batch: 360/2170 - Elapsed Time: 1105.78s - Remaining Time: 5559.63s - Train Acc: 0.5437\n",
      "Batch: 370/2170 - Elapsed Time: 1135.52s - Remaining Time: 5524.13s - Train Acc: 0.5436\n",
      "Batch: 380/2170 - Elapsed Time: 1164.81s - Remaining Time: 5486.86s - Train Acc: 0.5439\n",
      "Batch: 390/2170 - Elapsed Time: 1194.63s - Remaining Time: 5452.44s - Train Acc: 0.5452\n",
      "Batch: 400/2170 - Elapsed Time: 1223.88s - Remaining Time: 5415.67s - Train Acc: 0.5470\n",
      "Batch: 410/2170 - Elapsed Time: 1253.57s - Remaining Time: 5381.19s - Train Acc: 0.5488\n",
      "Batch: 420/2170 - Elapsed Time: 1283.06s - Remaining Time: 5346.10s - Train Acc: 0.5509\n",
      "Batch: 430/2170 - Elapsed Time: 1312.65s - Remaining Time: 5311.65s - Train Acc: 0.5529\n",
      "Batch: 440/2170 - Elapsed Time: 1342.11s - Remaining Time: 5276.91s - Train Acc: 0.5538\n",
      "Batch: 450/2170 - Elapsed Time: 1371.88s - Remaining Time: 5243.63s - Train Acc: 0.5561\n",
      "Batch: 460/2170 - Elapsed Time: 1400.98s - Remaining Time: 5207.98s - Train Acc: 0.5561\n",
      "Batch: 470/2170 - Elapsed Time: 1430.45s - Remaining Time: 5173.96s - Train Acc: 0.5570\n",
      "Batch: 480/2170 - Elapsed Time: 1459.44s - Remaining Time: 5138.46s - Train Acc: 0.5582\n",
      "Batch: 490/2170 - Elapsed Time: 1488.69s - Remaining Time: 5104.10s - Train Acc: 0.5608\n",
      "Batch: 500/2170 - Elapsed Time: 1517.97s - Remaining Time: 5070.03s - Train Acc: 0.5611\n",
      "Batch: 510/2170 - Elapsed Time: 1547.19s - Remaining Time: 5035.95s - Train Acc: 0.5631\n",
      "Batch: 520/2170 - Elapsed Time: 1576.33s - Remaining Time: 5001.83s - Train Acc: 0.5651\n",
      "Batch: 530/2170 - Elapsed Time: 1605.92s - Remaining Time: 4969.27s - Train Acc: 0.5662\n",
      "Batch: 540/2170 - Elapsed Time: 1636.37s - Remaining Time: 4939.43s - Train Acc: 0.5670\n",
      "Batch: 550/2170 - Elapsed Time: 1666.43s - Remaining Time: 4908.39s - Train Acc: 0.5670\n",
      "Batch: 560/2170 - Elapsed Time: 1696.85s - Remaining Time: 4878.46s - Train Acc: 0.5692\n",
      "Batch: 570/2170 - Elapsed Time: 1727.80s - Remaining Time: 4849.95s - Train Acc: 0.5700\n",
      "Batch: 580/2170 - Elapsed Time: 1758.54s - Remaining Time: 4820.84s - Train Acc: 0.5702\n",
      "Batch: 590/2170 - Elapsed Time: 1788.66s - Remaining Time: 4789.96s - Train Acc: 0.5714\n",
      "Batch: 600/2170 - Elapsed Time: 1818.86s - Remaining Time: 4759.35s - Train Acc: 0.5711\n",
      "Batch: 610/2170 - Elapsed Time: 1849.76s - Remaining Time: 4730.53s - Train Acc: 0.5718\n",
      "Batch: 620/2170 - Elapsed Time: 1879.91s - Remaining Time: 4699.78s - Train Acc: 0.5729\n",
      "Batch: 630/2170 - Elapsed Time: 1910.27s - Remaining Time: 4669.55s - Train Acc: 0.5744\n",
      "Batch: 640/2170 - Elapsed Time: 1940.41s - Remaining Time: 4638.80s - Train Acc: 0.5767\n",
      "Batch: 650/2170 - Elapsed Time: 1971.04s - Remaining Time: 4609.21s - Train Acc: 0.5788\n",
      "Batch: 660/2170 - Elapsed Time: 2001.28s - Remaining Time: 4578.69s - Train Acc: 0.5800\n",
      "Batch: 670/2170 - Elapsed Time: 2031.49s - Remaining Time: 4548.11s - Train Acc: 0.5812\n",
      "Batch: 680/2170 - Elapsed Time: 2061.97s - Remaining Time: 4518.15s - Train Acc: 0.5815\n",
      "Batch: 690/2170 - Elapsed Time: 2092.32s - Remaining Time: 4487.87s - Train Acc: 0.5818\n",
      "Batch: 700/2170 - Elapsed Time: 2123.02s - Remaining Time: 4458.34s - Train Acc: 0.5826\n",
      "Batch: 710/2170 - Elapsed Time: 2153.23s - Remaining Time: 4427.77s - Train Acc: 0.5835\n",
      "Batch: 720/2170 - Elapsed Time: 2183.98s - Remaining Time: 4398.30s - Train Acc: 0.5843\n",
      "Batch: 730/2170 - Elapsed Time: 2215.81s - Remaining Time: 4370.91s - Train Acc: 0.5850\n",
      "Batch: 740/2170 - Elapsed Time: 2246.77s - Remaining Time: 4341.73s - Train Acc: 0.5857\n",
      "Batch: 750/2170 - Elapsed Time: 2277.38s - Remaining Time: 4311.83s - Train Acc: 0.5868\n",
      "Batch: 760/2170 - Elapsed Time: 2307.32s - Remaining Time: 4280.69s - Train Acc: 0.5891\n",
      "Batch: 770/2170 - Elapsed Time: 2341.55s - Remaining Time: 4257.37s - Train Acc: 0.5911\n",
      "Batch: 780/2170 - Elapsed Time: 2372.32s - Remaining Time: 4227.60s - Train Acc: 0.5924\n",
      "Batch: 790/2170 - Elapsed Time: 2403.02s - Remaining Time: 4197.69s - Train Acc: 0.5939\n",
      "Batch: 800/2170 - Elapsed Time: 2433.64s - Remaining Time: 4167.61s - Train Acc: 0.5949\n",
      "Batch: 810/2170 - Elapsed Time: 2463.78s - Remaining Time: 4136.72s - Train Acc: 0.5963\n",
      "Batch: 820/2170 - Elapsed Time: 2494.34s - Remaining Time: 4106.53s - Train Acc: 0.5969\n",
      "Batch: 830/2170 - Elapsed Time: 2524.58s - Remaining Time: 4075.84s - Train Acc: 0.5979\n",
      "Batch: 840/2170 - Elapsed Time: 2555.45s - Remaining Time: 4046.13s - Train Acc: 0.5999\n",
      "Batch: 850/2170 - Elapsed Time: 2585.71s - Remaining Time: 4015.45s - Train Acc: 0.6004\n",
      "Batch: 860/2170 - Elapsed Time: 2616.06s - Remaining Time: 3984.94s - Train Acc: 0.6015\n",
      "Batch: 870/2170 - Elapsed Time: 2646.67s - Remaining Time: 3954.79s - Train Acc: 0.6028\n",
      "Batch: 880/2170 - Elapsed Time: 2676.95s - Remaining Time: 3924.16s - Train Acc: 0.6042\n",
      "Batch: 890/2170 - Elapsed Time: 2707.85s - Remaining Time: 3894.44s - Train Acc: 0.6056\n",
      "Batch: 900/2170 - Elapsed Time: 2738.18s - Remaining Time: 3863.88s - Train Acc: 0.6072\n",
      "Batch: 910/2170 - Elapsed Time: 2768.71s - Remaining Time: 3833.59s - Train Acc: 0.6078\n",
      "Batch: 920/2170 - Elapsed Time: 2798.82s - Remaining Time: 3802.75s - Train Acc: 0.6094\n",
      "Batch: 930/2170 - Elapsed Time: 2829.69s - Remaining Time: 3772.92s - Train Acc: 0.6108\n",
      "Batch: 940/2170 - Elapsed Time: 2860.17s - Remaining Time: 3742.56s - Train Acc: 0.6116\n",
      "Batch: 950/2170 - Elapsed Time: 2890.44s - Remaining Time: 3711.93s - Train Acc: 0.6118\n",
      "Batch: 960/2170 - Elapsed Time: 2921.11s - Remaining Time: 3681.82s - Train Acc: 0.6130\n",
      "Batch: 970/2170 - Elapsed Time: 2951.91s - Remaining Time: 3651.85s - Train Acc: 0.6140\n",
      "Batch: 980/2170 - Elapsed Time: 2982.50s - Remaining Time: 3621.61s - Train Acc: 0.6148\n",
      "Batch: 990/2170 - Elapsed Time: 3012.46s - Remaining Time: 3590.60s - Train Acc: 0.6157\n",
      "Batch: 1000/2170 - Elapsed Time: 3043.04s - Remaining Time: 3560.36s - Train Acc: 0.6162\n",
      "Batch: 1010/2170 - Elapsed Time: 3073.49s - Remaining Time: 3529.95s - Train Acc: 0.6170\n",
      "Batch: 1020/2170 - Elapsed Time: 3103.70s - Remaining Time: 3499.26s - Train Acc: 0.6183\n",
      "Batch: 1030/2170 - Elapsed Time: 3135.53s - Remaining Time: 3470.39s - Train Acc: 0.6199\n",
      "Batch: 1040/2170 - Elapsed Time: 3165.65s - Remaining Time: 3439.60s - Train Acc: 0.6206\n",
      "Batch: 1050/2170 - Elapsed Time: 3195.79s - Remaining Time: 3408.84s - Train Acc: 0.6214\n",
      "Batch: 1060/2170 - Elapsed Time: 3225.40s - Remaining Time: 3377.54s - Train Acc: 0.6225\n",
      "Batch: 1070/2170 - Elapsed Time: 3255.68s - Remaining Time: 3346.96s - Train Acc: 0.6236\n",
      "Batch: 1080/2170 - Elapsed Time: 3285.61s - Remaining Time: 3316.03s - Train Acc: 0.6241\n",
      "Batch: 1090/2170 - Elapsed Time: 3315.73s - Remaining Time: 3285.31s - Train Acc: 0.6252\n",
      "Batch: 1100/2170 - Elapsed Time: 3346.17s - Remaining Time: 3254.91s - Train Acc: 0.6262\n",
      "Batch: 1110/2170 - Elapsed Time: 3375.88s - Remaining Time: 3223.81s - Train Acc: 0.6273\n",
      "Batch: 1120/2170 - Elapsed Time: 3405.91s - Remaining Time: 3193.04s - Train Acc: 0.6277\n",
      "Batch: 1130/2170 - Elapsed Time: 3435.84s - Remaining Time: 3162.19s - Train Acc: 0.6281\n",
      "Batch: 1140/2170 - Elapsed Time: 3465.99s - Remaining Time: 3131.56s - Train Acc: 0.6293\n",
      "Batch: 1150/2170 - Elapsed Time: 3495.81s - Remaining Time: 3100.63s - Train Acc: 0.6299\n",
      "Batch: 1160/2170 - Elapsed Time: 3526.67s - Remaining Time: 3070.64s - Train Acc: 0.6305\n",
      "Batch: 1170/2170 - Elapsed Time: 3556.56s - Remaining Time: 3039.80s - Train Acc: 0.6311\n",
      "Batch: 1180/2170 - Elapsed Time: 3586.22s - Remaining Time: 3008.78s - Train Acc: 0.6326\n",
      "Batch: 1190/2170 - Elapsed Time: 3616.33s - Remaining Time: 2978.15s - Train Acc: 0.6337\n",
      "Batch: 1200/2170 - Elapsed Time: 3646.01s - Remaining Time: 2947.19s - Train Acc: 0.6343\n",
      "Batch: 1210/2170 - Elapsed Time: 3676.04s - Remaining Time: 2916.53s - Train Acc: 0.6352\n",
      "Batch: 1220/2170 - Elapsed Time: 3708.15s - Remaining Time: 2887.49s - Train Acc: 0.6360\n",
      "Batch: 1230/2170 - Elapsed Time: 3738.24s - Remaining Time: 2856.86s - Train Acc: 0.6364\n",
      "Batch: 1240/2170 - Elapsed Time: 3767.84s - Remaining Time: 2825.88s - Train Acc: 0.6376\n",
      "Batch: 1250/2170 - Elapsed Time: 3797.55s - Remaining Time: 2794.99s - Train Acc: 0.6384\n",
      "Batch: 1260/2170 - Elapsed Time: 3826.80s - Remaining Time: 2763.80s - Train Acc: 0.6396\n",
      "Batch: 1270/2170 - Elapsed Time: 3856.33s - Remaining Time: 2732.83s - Train Acc: 0.6406\n",
      "Batch: 1280/2170 - Elapsed Time: 3885.46s - Remaining Time: 2701.61s - Train Acc: 0.6409\n",
      "Batch: 1290/2170 - Elapsed Time: 3914.97s - Remaining Time: 2670.68s - Train Acc: 0.6413\n",
      "Batch: 1300/2170 - Elapsed Time: 3944.08s - Remaining Time: 2639.50s - Train Acc: 0.6423\n",
      "Batch: 1310/2170 - Elapsed Time: 3974.21s - Remaining Time: 2609.02s - Train Acc: 0.6430\n",
      "Batch: 1320/2170 - Elapsed Time: 4003.85s - Remaining Time: 2578.24s - Train Acc: 0.6439\n",
      "Batch: 1330/2170 - Elapsed Time: 4033.20s - Remaining Time: 2547.29s - Train Acc: 0.6447\n",
      "Batch: 1340/2170 - Elapsed Time: 4062.74s - Remaining Time: 2516.47s - Train Acc: 0.6456\n",
      "Batch: 1350/2170 - Elapsed Time: 4091.78s - Remaining Time: 2485.38s - Train Acc: 0.6468\n",
      "Batch: 1360/2170 - Elapsed Time: 4121.43s - Remaining Time: 2454.68s - Train Acc: 0.6473\n",
      "Batch: 1370/2170 - Elapsed Time: 4150.82s - Remaining Time: 2423.84s - Train Acc: 0.6480\n",
      "Batch: 1380/2170 - Elapsed Time: 4180.17s - Remaining Time: 2393.00s - Train Acc: 0.6489\n",
      "Batch: 1390/2170 - Elapsed Time: 4209.23s - Remaining Time: 2362.01s - Train Acc: 0.6495\n",
      "Batch: 1400/2170 - Elapsed Time: 4238.63s - Remaining Time: 2331.25s - Train Acc: 0.6501\n",
      "Batch: 1410/2170 - Elapsed Time: 4267.66s - Remaining Time: 2300.30s - Train Acc: 0.6516\n",
      "Batch: 1420/2170 - Elapsed Time: 4296.90s - Remaining Time: 2269.49s - Train Acc: 0.6525\n",
      "Batch: 1430/2170 - Elapsed Time: 4325.89s - Remaining Time: 2238.57s - Train Acc: 0.6534\n",
      "Batch: 1440/2170 - Elapsed Time: 4355.69s - Remaining Time: 2208.09s - Train Acc: 0.6542\n",
      "Batch: 1450/2170 - Elapsed Time: 4384.98s - Remaining Time: 2177.37s - Train Acc: 0.6542\n",
      "Batch: 1460/2170 - Elapsed Time: 4414.44s - Remaining Time: 2146.75s - Train Acc: 0.6548\n",
      "Batch: 1470/2170 - Elapsed Time: 4444.50s - Remaining Time: 2116.43s - Train Acc: 0.6555\n",
      "Batch: 1480/2170 - Elapsed Time: 4475.21s - Remaining Time: 2086.41s - Train Acc: 0.6561\n",
      "Batch: 1490/2170 - Elapsed Time: 4504.35s - Remaining Time: 2055.68s - Train Acc: 0.6568\n",
      "Batch: 1500/2170 - Elapsed Time: 4534.15s - Remaining Time: 2025.25s - Train Acc: 0.6575\n",
      "Batch: 1510/2170 - Elapsed Time: 4563.10s - Remaining Time: 1994.47s - Train Acc: 0.6584\n",
      "Batch: 1520/2170 - Elapsed Time: 4592.55s - Remaining Time: 1963.92s - Train Acc: 0.6587\n",
      "Batch: 1530/2170 - Elapsed Time: 4621.51s - Remaining Time: 1933.18s - Train Acc: 0.6590\n",
      "Batch: 1540/2170 - Elapsed Time: 4651.40s - Remaining Time: 1902.84s - Train Acc: 0.6596\n",
      "Batch: 1550/2170 - Elapsed Time: 4680.55s - Remaining Time: 1872.22s - Train Acc: 0.6604\n",
      "Batch: 1560/2170 - Elapsed Time: 4710.18s - Remaining Time: 1841.80s - Train Acc: 0.6614\n",
      "Batch: 1570/2170 - Elapsed Time: 4739.25s - Remaining Time: 1811.18s - Train Acc: 0.6623\n",
      "Batch: 1580/2170 - Elapsed Time: 4769.07s - Remaining Time: 1780.86s - Train Acc: 0.6634\n",
      "Batch: 1590/2170 - Elapsed Time: 4797.96s - Remaining Time: 1750.20s - Train Acc: 0.6643\n",
      "Batch: 1600/2170 - Elapsed Time: 4827.41s - Remaining Time: 1719.76s - Train Acc: 0.6649\n",
      "Batch: 1610/2170 - Elapsed Time: 4856.47s - Remaining Time: 1689.21s - Train Acc: 0.6660\n",
      "Batch: 1620/2170 - Elapsed Time: 4885.82s - Remaining Time: 1658.77s - Train Acc: 0.6660\n",
      "Batch: 1630/2170 - Elapsed Time: 4914.87s - Remaining Time: 1628.24s - Train Acc: 0.6668\n",
      "Batch: 1640/2170 - Elapsed Time: 4944.21s - Remaining Time: 1597.83s - Train Acc: 0.6675\n",
      "Batch: 1650/2170 - Elapsed Time: 4973.69s - Remaining Time: 1567.47s - Train Acc: 0.6680\n",
      "Batch: 1660/2170 - Elapsed Time: 5002.75s - Remaining Time: 1536.99s - Train Acc: 0.6685\n",
      "Batch: 1670/2170 - Elapsed Time: 5032.03s - Remaining Time: 1506.59s - Train Acc: 0.6689\n",
      "Batch: 1680/2170 - Elapsed Time: 5061.66s - Remaining Time: 1476.32s - Train Acc: 0.6698\n",
      "Batch: 1690/2170 - Elapsed Time: 5229.39s - Remaining Time: 1485.27s - Train Acc: 0.6705\n",
      "Batch: 1700/2170 - Elapsed Time: 5254.33s - Remaining Time: 1452.67s - Train Acc: 0.6713\n",
      "Batch: 1710/2170 - Elapsed Time: 5278.37s - Remaining Time: 1419.91s - Train Acc: 0.6721\n",
      "Batch: 1720/2170 - Elapsed Time: 5303.19s - Remaining Time: 1387.46s - Train Acc: 0.6721\n",
      "Batch: 1730/2170 - Elapsed Time: 5328.76s - Remaining Time: 1355.29s - Train Acc: 0.6728\n",
      "Batch: 1740/2170 - Elapsed Time: 5356.64s - Remaining Time: 1323.77s - Train Acc: 0.6733\n",
      "Batch: 1750/2170 - Elapsed Time: 5384.61s - Remaining Time: 1292.31s - Train Acc: 0.6743\n",
      "Batch: 1760/2170 - Elapsed Time: 5417.59s - Remaining Time: 1262.05s - Train Acc: 0.6751\n",
      "Batch: 1770/2170 - Elapsed Time: 5453.09s - Remaining Time: 1232.34s - Train Acc: 0.6759\n",
      "Batch: 1780/2170 - Elapsed Time: 5492.97s - Remaining Time: 1203.52s - Train Acc: 0.6765\n",
      "Batch: 1790/2170 - Elapsed Time: 5536.98s - Remaining Time: 1175.45s - Train Acc: 0.6770\n",
      "Batch: 1800/2170 - Elapsed Time: 5563.34s - Remaining Time: 1143.58s - Train Acc: 0.6774\n",
      "Batch: 1810/2170 - Elapsed Time: 5584.80s - Remaining Time: 1110.79s - Train Acc: 0.6776\n",
      "Batch: 1820/2170 - Elapsed Time: 5606.68s - Remaining Time: 1078.21s - Train Acc: 0.6783\n",
      "Batch: 1830/2170 - Elapsed Time: 5628.97s - Remaining Time: 1045.82s - Train Acc: 0.6788\n",
      "Batch: 1840/2170 - Elapsed Time: 5651.71s - Remaining Time: 1013.62s - Train Acc: 0.6794\n",
      "Batch: 1850/2170 - Elapsed Time: 5675.14s - Remaining Time: 981.65s - Train Acc: 0.6802\n",
      "Batch: 1860/2170 - Elapsed Time: 5698.88s - Remaining Time: 949.81s - Train Acc: 0.6808\n",
      "Batch: 1870/2170 - Elapsed Time: 5723.55s - Remaining Time: 918.22s - Train Acc: 0.6810\n",
      "Batch: 1880/2170 - Elapsed Time: 5749.10s - Remaining Time: 886.83s - Train Acc: 0.6811\n",
      "Batch: 1890/2170 - Elapsed Time: 5775.18s - Remaining Time: 855.58s - Train Acc: 0.6811\n",
      "Batch: 1900/2170 - Elapsed Time: 5802.70s - Remaining Time: 824.59s - Train Acc: 0.6814\n",
      "Batch: 1910/2170 - Elapsed Time: 5831.60s - Remaining Time: 793.83s - Train Acc: 0.6816\n",
      "Batch: 1920/2170 - Elapsed Time: 5862.88s - Remaining Time: 763.40s - Train Acc: 0.6820\n",
      "Batch: 1930/2170 - Elapsed Time: 5894.57s - Remaining Time: 733.00s - Train Acc: 0.6827\n",
      "Batch: 1940/2170 - Elapsed Time: 5926.67s - Remaining Time: 702.65s - Train Acc: 0.6833\n",
      "Batch: 1950/2170 - Elapsed Time: 5958.33s - Remaining Time: 672.22s - Train Acc: 0.6836\n",
      "Batch: 1960/2170 - Elapsed Time: 5990.38s - Remaining Time: 641.83s - Train Acc: 0.6839\n",
      "Batch: 1970/2170 - Elapsed Time: 6022.29s - Remaining Time: 611.40s - Train Acc: 0.6845\n",
      "Batch: 1980/2170 - Elapsed Time: 6053.91s - Remaining Time: 580.93s - Train Acc: 0.6853\n",
      "Batch: 1990/2170 - Elapsed Time: 6085.78s - Remaining Time: 550.47s - Train Acc: 0.6859\n",
      "Batch: 2000/2170 - Elapsed Time: 6117.45s - Remaining Time: 519.98s - Train Acc: 0.6863\n",
      "Batch: 2010/2170 - Elapsed Time: 6149.16s - Remaining Time: 489.49s - Train Acc: 0.6872\n",
      "Batch: 2020/2170 - Elapsed Time: 6181.25s - Remaining Time: 459.00s - Train Acc: 0.6875\n",
      "Batch: 2030/2170 - Elapsed Time: 6212.80s - Remaining Time: 428.47s - Train Acc: 0.6883\n",
      "Batch: 2040/2170 - Elapsed Time: 6244.65s - Remaining Time: 397.94s - Train Acc: 0.6888\n",
      "Batch: 2050/2170 - Elapsed Time: 6276.71s - Remaining Time: 367.42s - Train Acc: 0.6892\n",
      "Batch: 2060/2170 - Elapsed Time: 6308.43s - Remaining Time: 336.86s - Train Acc: 0.6896\n",
      "Batch: 2070/2170 - Elapsed Time: 6340.74s - Remaining Time: 306.32s - Train Acc: 0.6901\n",
      "Batch: 2080/2170 - Elapsed Time: 6372.41s - Remaining Time: 275.73s - Train Acc: 0.6907\n",
      "Batch: 2090/2170 - Elapsed Time: 6404.07s - Remaining Time: 245.13s - Train Acc: 0.6912\n",
      "Batch: 2100/2170 - Elapsed Time: 6436.06s - Remaining Time: 214.54s - Train Acc: 0.6917\n",
      "Batch: 2110/2170 - Elapsed Time: 6468.35s - Remaining Time: 183.93s - Train Acc: 0.6922\n",
      "Batch: 2120/2170 - Elapsed Time: 6501.10s - Remaining Time: 153.33s - Train Acc: 0.6927\n",
      "Batch: 2130/2170 - Elapsed Time: 6533.14s - Remaining Time: 122.69s - Train Acc: 0.6931\n",
      "Batch: 2140/2170 - Elapsed Time: 6564.53s - Remaining Time: 92.03s - Train Acc: 0.6937\n",
      "Batch: 2150/2170 - Elapsed Time: 6596.30s - Remaining Time: 61.36s - Train Acc: 0.6941\n",
      "Batch: 2160/2170 - Elapsed Time: 6628.60s - Remaining Time: 30.69s - Train Acc: 0.6945\n",
      "Batch: 2170/2170 - Elapsed Time: 6659.21s - Remaining Time: 0.00s - Train Acc: 0.6947\n",
      "Train Loss: 0.0365, Val Loss: 0.0591\n",
      "Train Acc: 0.6949, Val Acc: 0.0625\n",
      "Epoch: 2/2\n",
      "Batch: 10/2170 - Elapsed Time: 32.76s - Remaining Time: 7076.03s - Train Acc: 0.8000\n",
      "Batch: 20/2170 - Elapsed Time: 64.46s - Remaining Time: 6929.45s - Train Acc: 0.8094\n",
      "Batch: 30/2170 - Elapsed Time: 96.04s - Remaining Time: 6851.09s - Train Acc: 0.8063\n",
      "Batch: 40/2170 - Elapsed Time: 127.77s - Remaining Time: 6803.61s - Train Acc: 0.7937\n",
      "Batch: 50/2170 - Elapsed Time: 159.08s - Remaining Time: 6744.87s - Train Acc: 0.7925\n",
      "Batch: 60/2170 - Elapsed Time: 190.71s - Remaining Time: 6706.53s - Train Acc: 0.7844\n",
      "Batch: 70/2170 - Elapsed Time: 222.78s - Remaining Time: 6683.27s - Train Acc: 0.7750\n",
      "Batch: 80/2170 - Elapsed Time: 254.31s - Remaining Time: 6643.95s - Train Acc: 0.7797\n",
      "Batch: 90/2170 - Elapsed Time: 286.36s - Remaining Time: 6618.19s - Train Acc: 0.7785\n",
      "Batch: 100/2170 - Elapsed Time: 318.12s - Remaining Time: 6585.09s - Train Acc: 0.7887\n",
      "Batch: 110/2170 - Elapsed Time: 349.74s - Remaining Time: 6549.62s - Train Acc: 0.7864\n",
      "Batch: 120/2170 - Elapsed Time: 381.42s - Remaining Time: 6515.94s - Train Acc: 0.7927\n",
      "Batch: 130/2170 - Elapsed Time: 412.67s - Remaining Time: 6475.79s - Train Acc: 0.7928\n",
      "Batch: 140/2170 - Elapsed Time: 444.39s - Remaining Time: 6443.60s - Train Acc: 0.7924\n",
      "Batch: 150/2170 - Elapsed Time: 475.92s - Remaining Time: 6409.09s - Train Acc: 0.7929\n",
      "Batch: 160/2170 - Elapsed Time: 507.78s - Remaining Time: 6378.97s - Train Acc: 0.7918\n",
      "Batch: 170/2170 - Elapsed Time: 539.79s - Remaining Time: 6350.44s - Train Acc: 0.7930\n",
      "Batch: 180/2170 - Elapsed Time: 571.21s - Remaining Time: 6315.08s - Train Acc: 0.7941\n",
      "Batch: 190/2170 - Elapsed Time: 602.93s - Remaining Time: 6283.18s - Train Acc: 0.7918\n",
      "Batch: 200/2170 - Elapsed Time: 634.58s - Remaining Time: 6250.62s - Train Acc: 0.7937\n",
      "Batch: 210/2170 - Elapsed Time: 665.89s - Remaining Time: 6214.96s - Train Acc: 0.7923\n",
      "Batch: 220/2170 - Elapsed Time: 697.72s - Remaining Time: 6184.36s - Train Acc: 0.7943\n",
      "Batch: 230/2170 - Elapsed Time: 729.10s - Remaining Time: 6149.79s - Train Acc: 0.7965\n",
      "Batch: 240/2170 - Elapsed Time: 760.69s - Remaining Time: 6117.24s - Train Acc: 0.7984\n",
      "Batch: 250/2170 - Elapsed Time: 792.34s - Remaining Time: 6085.16s - Train Acc: 0.8003\n",
      "Batch: 260/2170 - Elapsed Time: 823.93s - Remaining Time: 6052.70s - Train Acc: 0.7993\n",
      "Batch: 270/2170 - Elapsed Time: 855.69s - Remaining Time: 6021.50s - Train Acc: 0.7963\n",
      "Batch: 280/2170 - Elapsed Time: 887.44s - Remaining Time: 5990.21s - Train Acc: 0.7975\n",
      "Batch: 290/2170 - Elapsed Time: 918.95s - Remaining Time: 5957.33s - Train Acc: 0.7972\n",
      "Batch: 300/2170 - Elapsed Time: 950.63s - Remaining Time: 5925.58s - Train Acc: 0.7969\n",
      "Batch: 310/2170 - Elapsed Time: 981.85s - Remaining Time: 5891.08s - Train Acc: 0.7968\n",
      "Batch: 320/2170 - Elapsed Time: 1013.58s - Remaining Time: 5859.76s - Train Acc: 0.7963\n",
      "Batch: 330/2170 - Elapsed Time: 1045.31s - Remaining Time: 5828.42s - Train Acc: 0.7968\n",
      "Batch: 340/2170 - Elapsed Time: 1076.52s - Remaining Time: 5794.19s - Train Acc: 0.7967\n",
      "Batch: 350/2170 - Elapsed Time: 1108.10s - Remaining Time: 5762.14s - Train Acc: 0.7980\n",
      "Batch: 360/2170 - Elapsed Time: 1139.39s - Remaining Time: 5728.58s - Train Acc: 0.7995\n",
      "Batch: 370/2170 - Elapsed Time: 1171.32s - Remaining Time: 5698.33s - Train Acc: 0.7995\n",
      "Batch: 380/2170 - Elapsed Time: 1203.20s - Remaining Time: 5667.73s - Train Acc: 0.8002\n",
      "Batch: 390/2170 - Elapsed Time: 1234.42s - Remaining Time: 5634.02s - Train Acc: 0.7997\n",
      "Batch: 400/2170 - Elapsed Time: 1266.10s - Remaining Time: 5602.48s - Train Acc: 0.7983\n",
      "Batch: 410/2170 - Elapsed Time: 1297.51s - Remaining Time: 5569.81s - Train Acc: 0.7989\n",
      "Batch: 420/2170 - Elapsed Time: 1329.14s - Remaining Time: 5538.07s - Train Acc: 0.7990\n",
      "Batch: 430/2170 - Elapsed Time: 1360.79s - Remaining Time: 5506.47s - Train Acc: 0.8004\n",
      "Batch: 440/2170 - Elapsed Time: 1392.21s - Remaining Time: 5473.93s - Train Acc: 0.7993\n",
      "Batch: 450/2170 - Elapsed Time: 1423.82s - Remaining Time: 5442.15s - Train Acc: 0.7997\n",
      "Batch: 460/2170 - Elapsed Time: 1455.86s - Remaining Time: 5412.00s - Train Acc: 0.7996\n",
      "Batch: 470/2170 - Elapsed Time: 1487.35s - Remaining Time: 5379.79s - Train Acc: 0.8005\n",
      "Batch: 480/2170 - Elapsed Time: 1519.22s - Remaining Time: 5348.92s - Train Acc: 0.7997\n",
      "Batch: 490/2170 - Elapsed Time: 1550.59s - Remaining Time: 5316.31s - Train Acc: 0.8013\n",
      "Batch: 500/2170 - Elapsed Time: 1582.34s - Remaining Time: 5285.01s - Train Acc: 0.8011\n",
      "Batch: 510/2170 - Elapsed Time: 1613.97s - Remaining Time: 5253.31s - Train Acc: 0.8005\n",
      "Batch: 520/2170 - Elapsed Time: 1645.70s - Remaining Time: 5221.94s - Train Acc: 0.8011\n",
      "Batch: 530/2170 - Elapsed Time: 1677.45s - Remaining Time: 5190.61s - Train Acc: 0.8012\n",
      "Batch: 540/2170 - Elapsed Time: 1708.81s - Remaining Time: 5158.07s - Train Acc: 0.8010\n",
      "Batch: 550/2170 - Elapsed Time: 1740.47s - Remaining Time: 5126.48s - Train Acc: 0.8009\n",
      "Batch: 560/2170 - Elapsed Time: 1772.31s - Remaining Time: 5095.40s - Train Acc: 0.8007\n",
      "Batch: 570/2170 - Elapsed Time: 1803.80s - Remaining Time: 5063.31s - Train Acc: 0.8009\n",
      "Batch: 580/2170 - Elapsed Time: 1836.28s - Remaining Time: 5033.94s - Train Acc: 0.8014\n",
      "Batch: 590/2170 - Elapsed Time: 1868.13s - Remaining Time: 5002.80s - Train Acc: 0.8007\n",
      "Batch: 600/2170 - Elapsed Time: 1899.75s - Remaining Time: 4971.00s - Train Acc: 0.8002\n",
      "Batch: 610/2170 - Elapsed Time: 1931.44s - Remaining Time: 4939.42s - Train Acc: 0.7998\n",
      "Batch: 620/2170 - Elapsed Time: 1962.65s - Remaining Time: 4906.64s - Train Acc: 0.8002\n",
      "Batch: 630/2170 - Elapsed Time: 1994.26s - Remaining Time: 4874.86s - Train Acc: 0.8010\n",
      "Batch: 640/2170 - Elapsed Time: 2025.93s - Remaining Time: 4843.23s - Train Acc: 0.8004\n",
      "Batch: 650/2170 - Elapsed Time: 2057.39s - Remaining Time: 4811.13s - Train Acc: 0.8010\n",
      "Batch: 660/2170 - Elapsed Time: 2088.92s - Remaining Time: 4779.21s - Train Acc: 0.8006\n",
      "Batch: 670/2170 - Elapsed Time: 2120.92s - Remaining Time: 4748.33s - Train Acc: 0.8007\n",
      "Batch: 680/2170 - Elapsed Time: 2152.32s - Remaining Time: 4716.12s - Train Acc: 0.8007\n",
      "Batch: 690/2170 - Elapsed Time: 2183.95s - Remaining Time: 4684.42s - Train Acc: 0.8012\n",
      "Batch: 700/2170 - Elapsed Time: 2215.22s - Remaining Time: 4651.97s - Train Acc: 0.8019\n",
      "Batch: 710/2170 - Elapsed Time: 2247.23s - Remaining Time: 4621.06s - Train Acc: 0.8031\n",
      "Batch: 720/2170 - Elapsed Time: 2279.10s - Remaining Time: 4589.86s - Train Acc: 0.8022\n",
      "Batch: 730/2170 - Elapsed Time: 2310.27s - Remaining Time: 4557.25s - Train Acc: 0.8026\n",
      "Batch: 740/2170 - Elapsed Time: 2341.94s - Remaining Time: 4525.64s - Train Acc: 0.8024\n",
      "Batch: 750/2170 - Elapsed Time: 2373.33s - Remaining Time: 4493.50s - Train Acc: 0.8024\n",
      "Batch: 760/2170 - Elapsed Time: 2405.19s - Remaining Time: 4462.26s - Train Acc: 0.8021\n",
      "Batch: 770/2170 - Elapsed Time: 2436.80s - Remaining Time: 4430.55s - Train Acc: 0.8016\n",
      "Batch: 780/2170 - Elapsed Time: 2468.10s - Remaining Time: 4398.27s - Train Acc: 0.8022\n",
      "Batch: 790/2170 - Elapsed Time: 2499.67s - Remaining Time: 4366.51s - Train Acc: 0.8019\n",
      "Batch: 800/2170 - Elapsed Time: 2530.91s - Remaining Time: 4334.19s - Train Acc: 0.8023\n",
      "Batch: 810/2170 - Elapsed Time: 2563.32s - Remaining Time: 4303.85s - Train Acc: 0.8021\n",
      "Batch: 820/2170 - Elapsed Time: 2597.85s - Remaining Time: 4276.95s - Train Acc: 0.8022\n",
      "Batch: 830/2170 - Elapsed Time: 2632.21s - Remaining Time: 4249.60s - Train Acc: 0.8023\n",
      "Batch: 840/2170 - Elapsed Time: 2668.63s - Remaining Time: 4225.33s - Train Acc: 0.8025\n",
      "Batch: 850/2170 - Elapsed Time: 2708.18s - Remaining Time: 4205.64s - Train Acc: 0.8020\n",
      "Batch: 860/2170 - Elapsed Time: 2743.21s - Remaining Time: 4178.60s - Train Acc: 0.8025\n",
      "Batch: 870/2170 - Elapsed Time: 2774.34s - Remaining Time: 4145.56s - Train Acc: 0.8028\n",
      "Batch: 880/2170 - Elapsed Time: 2806.20s - Remaining Time: 4113.63s - Train Acc: 0.8033\n",
      "Batch: 890/2170 - Elapsed Time: 2837.52s - Remaining Time: 4080.92s - Train Acc: 0.8027\n",
      "Batch: 900/2170 - Elapsed Time: 2868.50s - Remaining Time: 4047.78s - Train Acc: 0.8026\n",
      "Batch: 910/2170 - Elapsed Time: 2900.04s - Remaining Time: 4015.43s - Train Acc: 0.8032\n",
      "Batch: 920/2170 - Elapsed Time: 2930.80s - Remaining Time: 3982.06s - Train Acc: 0.8029\n",
      "Batch: 930/2170 - Elapsed Time: 2962.29s - Remaining Time: 3949.71s - Train Acc: 0.8030\n",
      "Batch: 940/2170 - Elapsed Time: 2993.52s - Remaining Time: 3917.05s - Train Acc: 0.8033\n",
      "Batch: 950/2170 - Elapsed Time: 3024.72s - Remaining Time: 3884.37s - Train Acc: 0.8035\n",
      "Batch: 960/2170 - Elapsed Time: 3055.92s - Remaining Time: 3851.73s - Train Acc: 0.8030\n",
      "Batch: 970/2170 - Elapsed Time: 3086.87s - Remaining Time: 3818.80s - Train Acc: 0.8028\n",
      "Batch: 980/2170 - Elapsed Time: 3118.16s - Remaining Time: 3786.33s - Train Acc: 0.8034\n",
      "Batch: 990/2170 - Elapsed Time: 3149.36s - Remaining Time: 3753.78s - Train Acc: 0.8032\n",
      "Batch: 1000/2170 - Elapsed Time: 3180.54s - Remaining Time: 3721.24s - Train Acc: 0.8031\n",
      "Batch: 1010/2170 - Elapsed Time: 3211.88s - Remaining Time: 3688.89s - Train Acc: 0.8035\n",
      "Batch: 1020/2170 - Elapsed Time: 3242.73s - Remaining Time: 3656.02s - Train Acc: 0.8036\n",
      "Batch: 1030/2170 - Elapsed Time: 3274.38s - Remaining Time: 3624.07s - Train Acc: 0.8037\n",
      "Batch: 1040/2170 - Elapsed Time: 3305.61s - Remaining Time: 3591.67s - Train Acc: 0.8034\n",
      "Batch: 1050/2170 - Elapsed Time: 3336.91s - Remaining Time: 3559.37s - Train Acc: 0.8039\n",
      "Batch: 1060/2170 - Elapsed Time: 3368.29s - Remaining Time: 3527.17s - Train Acc: 0.8040\n",
      "Batch: 1070/2170 - Elapsed Time: 3399.28s - Remaining Time: 3494.59s - Train Acc: 0.8037\n",
      "Batch: 1080/2170 - Elapsed Time: 3430.54s - Remaining Time: 3462.31s - Train Acc: 0.8038\n",
      "Batch: 1090/2170 - Elapsed Time: 3461.64s - Remaining Time: 3429.88s - Train Acc: 0.8034\n",
      "Batch: 1100/2170 - Elapsed Time: 3493.03s - Remaining Time: 3397.77s - Train Acc: 0.8037\n",
      "Batch: 1110/2170 - Elapsed Time: 3524.16s - Remaining Time: 3365.42s - Train Acc: 0.8041\n",
      "Batch: 1120/2170 - Elapsed Time: 3557.64s - Remaining Time: 3335.29s - Train Acc: 0.8038\n",
      "Batch: 1130/2170 - Elapsed Time: 3589.34s - Remaining Time: 3303.47s - Train Acc: 0.8035\n",
      "Batch: 1140/2170 - Elapsed Time: 3620.30s - Remaining Time: 3270.97s - Train Acc: 0.8035\n",
      "Batch: 1150/2170 - Elapsed Time: 3652.49s - Remaining Time: 3239.60s - Train Acc: 0.8034\n",
      "Batch: 1160/2170 - Elapsed Time: 3683.80s - Remaining Time: 3207.44s - Train Acc: 0.8038\n",
      "Batch: 1170/2170 - Elapsed Time: 3714.81s - Remaining Time: 3175.05s - Train Acc: 0.8038\n",
      "Batch: 1180/2170 - Elapsed Time: 3746.11s - Remaining Time: 3142.92s - Train Acc: 0.8042\n",
      "Batch: 1190/2170 - Elapsed Time: 3776.82s - Remaining Time: 3110.32s - Train Acc: 0.8049\n",
      "Batch: 1200/2170 - Elapsed Time: 3808.19s - Remaining Time: 3078.28s - Train Acc: 0.8052\n",
      "Batch: 1210/2170 - Elapsed Time: 3839.52s - Remaining Time: 3046.23s - Train Acc: 0.8054\n",
      "Batch: 1220/2170 - Elapsed Time: 3871.03s - Remaining Time: 3014.33s - Train Acc: 0.8056\n",
      "Batch: 1230/2170 - Elapsed Time: 3902.23s - Remaining Time: 2982.19s - Train Acc: 0.8058\n",
      "Batch: 1240/2170 - Elapsed Time: 3933.21s - Remaining Time: 2949.91s - Train Acc: 0.8062\n",
      "Batch: 1250/2170 - Elapsed Time: 3964.48s - Remaining Time: 2917.85s - Train Acc: 0.8064\n",
      "Batch: 1260/2170 - Elapsed Time: 3995.54s - Remaining Time: 2885.67s - Train Acc: 0.8065\n",
      "Batch: 1270/2170 - Elapsed Time: 4026.54s - Remaining Time: 2853.46s - Train Acc: 0.8069\n",
      "Batch: 1280/2170 - Elapsed Time: 4057.92s - Remaining Time: 2821.53s - Train Acc: 0.8068\n",
      "Batch: 1290/2170 - Elapsed Time: 4088.70s - Remaining Time: 2789.19s - Train Acc: 0.8068\n",
      "Batch: 1300/2170 - Elapsed Time: 4120.06s - Remaining Time: 2757.27s - Train Acc: 0.8065\n",
      "Batch: 1310/2170 - Elapsed Time: 4151.03s - Remaining Time: 2725.10s - Train Acc: 0.8065\n",
      "Batch: 1320/2170 - Elapsed Time: 4182.73s - Remaining Time: 2693.42s - Train Acc: 0.8072\n",
      "Batch: 1330/2170 - Elapsed Time: 4214.13s - Remaining Time: 2661.56s - Train Acc: 0.8070\n",
      "Batch: 1340/2170 - Elapsed Time: 4245.04s - Remaining Time: 2629.39s - Train Acc: 0.8073\n",
      "Batch: 1350/2170 - Elapsed Time: 4276.33s - Remaining Time: 2597.47s - Train Acc: 0.8072\n",
      "Batch: 1360/2170 - Elapsed Time: 4307.24s - Remaining Time: 2565.34s - Train Acc: 0.8071\n",
      "Batch: 1370/2170 - Elapsed Time: 4338.51s - Remaining Time: 2533.44s - Train Acc: 0.8073\n",
      "Batch: 1380/2170 - Elapsed Time: 4369.71s - Remaining Time: 2501.50s - Train Acc: 0.8077\n",
      "Batch: 1390/2170 - Elapsed Time: 4400.73s - Remaining Time: 2469.48s - Train Acc: 0.8076\n",
      "Batch: 1400/2170 - Elapsed Time: 4432.16s - Remaining Time: 2437.69s - Train Acc: 0.8074\n",
      "Batch: 1410/2170 - Elapsed Time: 4463.21s - Remaining Time: 2405.70s - Train Acc: 0.8071\n",
      "Batch: 1420/2170 - Elapsed Time: 4494.38s - Remaining Time: 2373.79s - Train Acc: 0.8070\n",
      "Batch: 1430/2170 - Elapsed Time: 4525.56s - Remaining Time: 2341.90s - Train Acc: 0.8067\n",
      "Batch: 1440/2170 - Elapsed Time: 4556.70s - Remaining Time: 2309.99s - Train Acc: 0.8065\n",
      "Batch: 1450/2170 - Elapsed Time: 4588.22s - Remaining Time: 2278.29s - Train Acc: 0.8069\n",
      "Batch: 1460/2170 - Elapsed Time: 4619.10s - Remaining Time: 2246.27s - Train Acc: 0.8071\n",
      "Batch: 1470/2170 - Elapsed Time: 4650.43s - Remaining Time: 2214.49s - Train Acc: 0.8068\n",
      "Batch: 1480/2170 - Elapsed Time: 4681.35s - Remaining Time: 2182.52s - Train Acc: 0.8070\n",
      "Batch: 1490/2170 - Elapsed Time: 4712.67s - Remaining Time: 2150.75s - Train Acc: 0.8069\n",
      "Batch: 1500/2170 - Elapsed Time: 4743.86s - Remaining Time: 2118.92s - Train Acc: 0.8070\n",
      "Batch: 1510/2170 - Elapsed Time: 4774.93s - Remaining Time: 2087.05s - Train Acc: 0.8070\n",
      "Batch: 1520/2170 - Elapsed Time: 4806.31s - Remaining Time: 2055.33s - Train Acc: 0.8072\n",
      "Batch: 1530/2170 - Elapsed Time: 4837.48s - Remaining Time: 2023.52s - Train Acc: 0.8076\n",
      "Batch: 1540/2170 - Elapsed Time: 4868.58s - Remaining Time: 1991.69s - Train Acc: 0.8078\n",
      "Batch: 1550/2170 - Elapsed Time: 4899.77s - Remaining Time: 1959.91s - Train Acc: 0.8077\n",
      "Batch: 1560/2170 - Elapsed Time: 4930.94s - Remaining Time: 1928.12s - Train Acc: 0.8077\n",
      "Batch: 1570/2170 - Elapsed Time: 4962.40s - Remaining Time: 1896.46s - Train Acc: 0.8078\n",
      "Batch: 1580/2170 - Elapsed Time: 4993.24s - Remaining Time: 1864.56s - Train Acc: 0.8078\n",
      "Batch: 1590/2170 - Elapsed Time: 5025.07s - Remaining Time: 1833.05s - Train Acc: 0.8075\n",
      "Batch: 1600/2170 - Elapsed Time: 5056.57s - Remaining Time: 1801.40s - Train Acc: 0.8075\n",
      "Batch: 1610/2170 - Elapsed Time: 5087.56s - Remaining Time: 1769.59s - Train Acc: 0.8079\n",
      "Batch: 1620/2170 - Elapsed Time: 5118.89s - Remaining Time: 1737.89s - Train Acc: 0.8079\n",
      "Batch: 1630/2170 - Elapsed Time: 5150.01s - Remaining Time: 1706.14s - Train Acc: 0.8077\n",
      "Batch: 1640/2170 - Elapsed Time: 5181.27s - Remaining Time: 1674.43s - Train Acc: 0.8077\n",
      "Batch: 1650/2170 - Elapsed Time: 5212.32s - Remaining Time: 1642.67s - Train Acc: 0.8081\n",
      "Batch: 1660/2170 - Elapsed Time: 5243.38s - Remaining Time: 1610.92s - Train Acc: 0.8085\n",
      "Batch: 1670/2170 - Elapsed Time: 5274.71s - Remaining Time: 1579.25s - Train Acc: 0.8085\n",
      "Batch: 1680/2170 - Elapsed Time: 5305.74s - Remaining Time: 1547.51s - Train Acc: 0.8086\n",
      "Batch: 1690/2170 - Elapsed Time: 5337.19s - Remaining Time: 1515.89s - Train Acc: 0.8087\n",
      "Batch: 1700/2170 - Elapsed Time: 5368.74s - Remaining Time: 1484.30s - Train Acc: 0.8089\n",
      "Batch: 1710/2170 - Elapsed Time: 5399.97s - Remaining Time: 1452.62s - Train Acc: 0.8090\n",
      "Batch: 1720/2170 - Elapsed Time: 5431.46s - Remaining Time: 1421.02s - Train Acc: 0.8090\n",
      "Batch: 1730/2170 - Elapsed Time: 5463.20s - Remaining Time: 1389.48s - Train Acc: 0.8088\n",
      "Batch: 1740/2170 - Elapsed Time: 5495.11s - Remaining Time: 1357.99s - Train Acc: 0.8087\n",
      "Batch: 1750/2170 - Elapsed Time: 5528.43s - Remaining Time: 1326.82s - Train Acc: 0.8090\n",
      "Batch: 1760/2170 - Elapsed Time: 5559.97s - Remaining Time: 1295.22s - Train Acc: 0.8093\n",
      "Batch: 1770/2170 - Elapsed Time: 5591.02s - Remaining Time: 1263.51s - Train Acc: 0.8097\n",
      "Batch: 1780/2170 - Elapsed Time: 5621.68s - Remaining Time: 1231.72s - Train Acc: 0.8099\n",
      "Batch: 1790/2170 - Elapsed Time: 5652.87s - Remaining Time: 1200.05s - Train Acc: 0.8097\n",
      "Batch: 1800/2170 - Elapsed Time: 5683.96s - Remaining Time: 1168.37s - Train Acc: 0.8099\n",
      "Batch: 1810/2170 - Elapsed Time: 5714.71s - Remaining Time: 1136.63s - Train Acc: 0.8101\n",
      "Batch: 1820/2170 - Elapsed Time: 5745.86s - Remaining Time: 1104.97s - Train Acc: 0.8102\n",
      "Batch: 1830/2170 - Elapsed Time: 5776.49s - Remaining Time: 1073.23s - Train Acc: 0.8101\n",
      "Batch: 1840/2170 - Elapsed Time: 5807.79s - Remaining Time: 1041.61s - Train Acc: 0.8103\n",
      "Batch: 1850/2170 - Elapsed Time: 5838.61s - Remaining Time: 1009.92s - Train Acc: 0.8106\n",
      "Batch: 1860/2170 - Elapsed Time: 5869.60s - Remaining Time: 978.27s - Train Acc: 0.8105\n",
      "Batch: 1870/2170 - Elapsed Time: 5900.59s - Remaining Time: 946.62s - Train Acc: 0.8106\n",
      "Batch: 1880/2170 - Elapsed Time: 5931.28s - Remaining Time: 914.93s - Train Acc: 0.8106\n",
      "Batch: 1890/2170 - Elapsed Time: 5962.27s - Remaining Time: 883.30s - Train Acc: 0.8105\n",
      "Batch: 1900/2170 - Elapsed Time: 5992.98s - Remaining Time: 851.63s - Train Acc: 0.8106\n",
      "Batch: 1910/2170 - Elapsed Time: 6024.06s - Remaining Time: 820.03s - Train Acc: 0.8106\n",
      "Batch: 1920/2170 - Elapsed Time: 6055.15s - Remaining Time: 788.43s - Train Acc: 0.8106\n",
      "Batch: 1930/2170 - Elapsed Time: 6085.69s - Remaining Time: 756.77s - Train Acc: 0.8107\n",
      "Batch: 1940/2170 - Elapsed Time: 6116.90s - Remaining Time: 725.20s - Train Acc: 0.8108\n",
      "Batch: 1950/2170 - Elapsed Time: 6147.43s - Remaining Time: 693.56s - Train Acc: 0.8109\n",
      "Batch: 1960/2170 - Elapsed Time: 6178.19s - Remaining Time: 661.95s - Train Acc: 0.8109\n",
      "Batch: 1970/2170 - Elapsed Time: 6224.06s - Remaining Time: 631.88s - Train Acc: 0.8110\n",
      "Batch: 1980/2170 - Elapsed Time: 6975.34s - Remaining Time: 669.35s - Train Acc: 0.8110\n",
      "Batch: 1990/2170 - Elapsed Time: 6997.01s - Remaining Time: 632.90s - Train Acc: 0.8111\n",
      "Batch: 2000/2170 - Elapsed Time: 7018.77s - Remaining Time: 596.60s - Train Acc: 0.8113\n",
      "Batch: 2010/2170 - Elapsed Time: 7040.60s - Remaining Time: 560.45s - Train Acc: 0.8112\n",
      "Batch: 2020/2170 - Elapsed Time: 7064.74s - Remaining Time: 524.61s - Train Acc: 0.8110\n",
      "Batch: 2030/2170 - Elapsed Time: 7088.66s - Remaining Time: 488.87s - Train Acc: 0.8109\n",
      "Batch: 2040/2170 - Elapsed Time: 7112.51s - Remaining Time: 453.25s - Train Acc: 0.8110\n",
      "Batch: 2050/2170 - Elapsed Time: 7136.63s - Remaining Time: 417.75s - Train Acc: 0.8108\n",
      "Batch: 2060/2170 - Elapsed Time: 7161.22s - Remaining Time: 382.40s - Train Acc: 0.8106\n",
      "Batch: 2070/2170 - Elapsed Time: 7186.92s - Remaining Time: 347.19s - Train Acc: 0.8107\n",
      "Batch: 2080/2170 - Elapsed Time: 7213.88s - Remaining Time: 312.14s - Train Acc: 0.8106\n",
      "Batch: 2090/2170 - Elapsed Time: 7242.51s - Remaining Time: 277.23s - Train Acc: 0.8106\n",
      "Batch: 2100/2170 - Elapsed Time: 7272.84s - Remaining Time: 242.43s - Train Acc: 0.8108\n",
      "Batch: 2110/2170 - Elapsed Time: 7302.87s - Remaining Time: 207.66s - Train Acc: 0.8107\n",
      "Batch: 2120/2170 - Elapsed Time: 7332.86s - Remaining Time: 172.94s - Train Acc: 0.8107\n",
      "Batch: 2130/2170 - Elapsed Time: 7362.76s - Remaining Time: 138.27s - Train Acc: 0.8107\n",
      "Batch: 2140/2170 - Elapsed Time: 7392.67s - Remaining Time: 103.64s - Train Acc: 0.8107\n",
      "Batch: 2150/2170 - Elapsed Time: 7425.66s - Remaining Time: 69.08s - Train Acc: 0.8107\n",
      "Batch: 2160/2170 - Elapsed Time: 7463.46s - Remaining Time: 34.55s - Train Acc: 0.8109\n",
      "Batch: 2170/2170 - Elapsed Time: 7500.49s - Remaining Time: 0.00s - Train Acc: 0.8105\n",
      "Train Loss: 0.0262, Val Loss: 0.0122\n",
      "Train Acc: 0.8108, Val Acc: 0.1876\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "    train_loss, val_loss = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    start_time = time.time()\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # move data to proper dtype and device\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=2).float() # convert labels to one-hot encoding\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0] # get raw logits from model\n",
    "\n",
    "        loss = loss_fn(logits, labels_one_hot) # compute loss using raw logits and one-hot encoded labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        train_acc += (pred == labels).sum().item()\n",
    "\n",
    "        # Print progress information\n",
    "        if (i+1) % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (i+1) * (len(train_loader) - (i+1))\n",
    "            current_acc = train_acc / ((i+1) * batch_size)\n",
    "            print(\"Batch: {}/{} - Elapsed Time: {:.2f}s - Remaining Time: {:.2f}s - Train Acc: {:.4f}\".format(i+1, len(train_loader), elapsed_time, remaining_time, current_acc))\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            input_ids = batch[\"input_ids\"].squeeze(1)\n",
    "            attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
    "            labels = batch[\"labels\"]\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=2).float() # convert labels to one-hot encoding\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[0] # get raw logits from model\n",
    "\n",
    "            loss = loss_fn(logits, labels_one_hot) # compute loss using raw logits and one-hot encoded labels\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            val_acc += (pred == labels).sum().item()\n",
    "\n",
    "    # Calculate the average losses and accuracies\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    train_acc = train_acc / len(train_dataset)\n",
    "    val_acc = val_acc / len(val_dataset)\n",
    "\n",
    "    # Print the losses and accuracies\n",
    "    print(\"Train Loss: {:.4f}, Val Loss: {:.4f}\".format(train_loss, val_loss))\n",
    "    print(\"Train Acc: {:.4f}, Val Acc: {:.4f}\".format(train_acc, val_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem unser Modell trainiert wurde, können wir seine Leistung bewerten, indem wir die Trainingsgenauigkeit über die Zeit plotten. Dazu öffnen wir eine Textdatei mit Trainingsfortschrittsinformationen und extrahieren die relevanten Daten mit regulären Ausdrücken. Anschließend plotten wir die Trainingsgenauigkeit für die Batches, um zu visualisieren, wie sie sich im Laufe der Zeit verändert. Aufgrund von Ressourcenbeschränkungen konnten wir nur 2 Epochen trainieren und mussten daher den Plot pro 10 Batches erstellen, anstatt pro Epoche. Die 2 trainierten Epochen haben auf unserer Hardware schon mehrere Stunden in Anspruch genommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0hklEQVR4nO3deVwU9f8H8NfuAst934hyKZ6IoiLeJglmpmWmpqKWWqZmmZXWN48uK8ufmVeZSuaZZnlUHuGVJ94nogiKCMspLPexO78/RlZXUAGBBfb1fDz2oTvzmZn37Cj75nNKBEEQQERERKRHpLoOgIiIiKi2MQEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7TICIiIhI7zABImpgxowZAw8PD12HQXruwIEDkEgk2LJli65DISoXEyCiWiKRSCr0OnDggK5DfaS///4bEokErq6uUKvVug6H7ilNNh582draonPnzli3bl2Vz7t06VKEh4dXX6BEdYiBrgMg0he//vqr1vs1a9Zg7969Zba3aNHiqa6zYsWKGktO1q1bBw8PD9y8eRP79u1DcHBwjVyHqubtt99Gx44dAQDp6enYtGkTRo4ciczMTEyaNKnS51u6dCns7e0xZsyYao6USPeYABHVkpEjR2q9P378OPbu3Vtm+8Py8vJgampa4esYGhpWKb4nyc3NxbZt2zBv3jysXr0a69atq7MJUG5uLszMzHQdRq3r3r07Xn75Zc37iRMnwsvLC+vXr69SAkTUkLEJjKgO6dWrF1q3bo3Tp0+jR48eMDU1xUcffQQA2LZtG/r37w9XV1fI5XJ4e3vjs88+g0ql0jrHw32Abt68CYlEgm+//RY//fQTvL29IZfL0bFjR5w8ebLCsf3xxx/Iz8/HkCFDMGzYMGzduhUFBQVlyhUUFGDOnDlo1qwZjI2N4eLigpdeegk3btzQlFGr1fj+++/Rpk0bGBsbw8HBAaGhoTh16pRWzOU1v0gkEsyZM0fzfs6cOZBIJLhy5QpeffVV2NjYoFu3bgCACxcuYMyYMfDy8oKxsTGcnZ3x2muvIT09vcx579y5g9dff13z+Xp6emLixIkoKipCbGwsJBIJ/u///q/McUePHoVEIsGGDRvK/dySk5NhYGCAuXPnltkXHR0NiUSCxYsXAwCKi4sxd+5cNG3aFMbGxrCzs0O3bt2wd+/ecs/9JEZGRrCxsYGBgfbvuqtXr8YzzzwDR0dHyOVytGzZEsuWLdMq4+HhgcuXL+PgwYOaZrVevXpp9mdmZuLdd9+Fh4cH5HI5GjVqhLCwMKSlpWmdR61W44svvkCjRo1gbGyMPn36ICYmpkr3Q1SdWANEVMekp6ejX79+GDZsGEaOHAknJycAQHh4OMzNzTFt2jSYm5tj3759mDVrFpRKJebPn//E865fvx7Z2dl44403IJFI8M033+Cll15CbGxshWqN1q1bh969e8PZ2RnDhg3DjBkzsGPHDgwZMkRTRqVS4fnnn0dERASGDRuGqVOnIjs7G3v37sWlS5fg7e0NAHj99dcRHh6Ofv36Ydy4cSgpKcF///2H48ePo0OHDlX63IYMGYKmTZviyy+/hCAIAIC9e/ciNjYWY8eOhbOzMy5fvoyffvoJly9fxvHjxyGRSAAAiYmJ6NSpEzIzMzFhwgQ0b94cd+7cwZYtW5CXlwcvLy907doV69atw7vvvlvmc7GwsMDAgQPLjcvJyQk9e/bEb7/9htmzZ2vt27RpE2QymeYznDNnDubNm4dx48ahU6dOUCqVOHXqFM6cOYNnn332iZ9Bdna2JgHJyMjA+vXrcenSJaxcuVKr3LJly9CqVSu88MILMDAwwI4dO/DWW29BrVZraooWLlyIKVOmwNzcHB9//LHmXgAgJycH3bt3R1RUFF577TW0b98eaWlp2L59OxISEmBvb6+51ldffQWpVIrp06cjKysL33zzDUaMGIETJ0488X6IapRARDoxadIk4eH/gj179hQACMuXLy9TPi8vr8y2N954QzA1NRUKCgo020aPHi00adJE8z4uLk4AINjZ2QkZGRma7du2bRMACDt27HhirMnJyYKBgYGwYsUKzbYuXboIAwcO1Cq3atUqAYCwYMGCMudQq9WCIAjCvn37BADC22+//cgypTGvXr26TBkAwuzZszXvZ8+eLQAQhg8fXqZseZ/Zhg0bBADCoUOHNNvCwsIEqVQqnDx58pEx/fjjjwIAISoqSrOvqKhIsLe3F0aPHl3muAeVHnvx4kWt7S1bthSeeeYZzfu2bdsK/fv3f+y5yrN//34BQJmXVCoVvvjiizLly/tcQkJCBC8vL61trVq1Enr27Fmm7KxZswQAwtatW8vsK/28SmNq0aKFUFhYqNn//fffl/tZENU2NoER1TFyuRxjx44ts93ExETz99Lf9Lt37468vDxcvXr1iecdOnQobGxsNO+7d+8OAIiNjX3isRs3boRUKsXgwYM124YPH45//vkHd+/e1Wz7/fffYW9vjylTppQ5R2lty++//w6JRFKmNuTBMlXx5ptvltn24GdWUFCAtLQ0dO7cGQBw5swZAGITzZ9//okBAwaUW/tUGtMrr7wCY2NjrVFVu3fvRlpa2hP7cb300kswMDDApk2bNNsuXbqEK1euYOjQoZpt1tbWuHz5Mq5fv16RWy5j1qxZ2Lt3L/bu3YtNmzZh+PDh+Pjjj/H9999rlXvwc8nKykJaWhp69uyJ2NhYZGVlPfE6v//+O9q2bYsXX3yxzL6Hn+HYsWNhZGSkeV+Zf3dENYkJEFEd4+bmpvWFUery5ct48cUXYWVlBUtLSzg4OGi+eCvypdW4cWOt96XJ0IMJzKOsXbsWnTp1Qnp6OmJiYhATE4N27dqhqKgImzdv1pS7ceMGfH19y/Q5edCNGzfg6uoKW1vbJ163Mjw9Pctsy8jIwNSpU+Hk5AQTExM4ODhoypV+ZqmpqVAqlWjduvVjz29tbY0BAwZg/fr1mm3r1q2Dm5sbnnnmmccea29vjz59+uC3337TbNu0aRMMDAzw0ksvabZ9+umnyMzMRLNmzdCmTRu8//77uHDhwpNv/p42bdogODgYwcHBeOWVV7B27Vo8//zzmDFjBlJTUzXljhw5guDgYJiZmcHa2hoODg6avmYV+bd048aNJ35epZ7m3x1RTWICRFTHPPjbeanMzEz07NkT58+fx6effoodO3Zg7969+PrrrwGgQsPeZTJZuduFe/1lHuX69es4efIkDh8+jKZNm2pepR2Nn2aemUd5VE3Qwx2+H1Te5/bKK69gxYoVePPNN7F161bs2bMHu3btAlCxz+xhYWFhiI2NxdGjR5GdnY3t27dj+PDhkEqf/KN02LBhuHbtGs6dOwcA+O2339CnTx+t/jI9evTAjRs3sGrVKrRu3Ro///wz2rdvj59//rnSsZbq06cPCgoKEBkZCUBMXvr06YO0tDQsWLAAf/31F/bu3avp21TdUyhU9d8dUU1jJ2iieuDAgQNIT0/H1q1b0aNHD832uLi4Gr/2unXrYGhoiF9//bXMl9nhw4exaNEixMfHo3HjxvD29saJEydQXFz8yI7V3t7e2L17NzIyMh5ZC1RaS5CZmam1/datWxWO++7du4iIiMDcuXMxa9YszfaHm5ccHBxgaWmJS5cuPfGcoaGhcHBwwLp16xAYGIi8vDyMGjWqQvEMGjQIb7zxhqYZ7Nq1a5g5c2aZcra2thg7dizGjh2LnJwc9OjRA3PmzMG4ceMqdJ2HlZSUABA7LgPAjh07UFhYiO3bt2vVzuzfv7/MsY9KRL29vSv0eRHVZawBIqoHShOPB39rLioqwtKlS2v82uvWrUP37t0xdOhQvPzyy1qv999/HwA0Q8AHDx6MtLQ0zbDuB5XGPnjwYAiCUO6w8NIylpaWsLe3x6FDh7T2V+Z+y/vMAHF004OkUikGDRqEHTt2aIbhlxcTABgYGGD48OH47bffEB4ejjZt2sDPz69C8VhbWyMkJAS//fYbNm7cCCMjIwwaNEirzMPD883NzeHj44PCwsIKXaM8O3fuBAC0bdsWQPmfS1ZWFlavXl3mWDMzszJJKCA+w/Pnz+OPP/4os481O1RfsAaIqB7o0qULbGxsMHr0aLz99tuQSCT49ddfa/zL5sSJE4iJicHkyZPL3e/m5ob27dtj3bp1+PDDDxEWFoY1a9Zg2rRpiIyMRPfu3ZGbm4t///0Xb731FgYOHIjevXtj1KhRWLRoEa5fv47Q0FCo1Wr8999/6N27t+Za48aNw1dffYVx48ahQ4cOOHToEK5du1bh2C0tLdGjRw988803KC4uhpubG/bs2VNurdmXX36JPXv2oGfPnpgwYQJatGiBpKQkbN68GYcPH4a1tbWmbFhYGBYtWoT9+/drmiAraujQoRg5ciSWLl2KkJAQrfMCQMuWLdGrVy8EBATA1tYWp06dwpYtWx75+T/sv//+08zNlJGRge3bt+PgwYMYNmwYmjdvDgDo27cvjIyMMGDAALzxxhvIycnBihUr4OjoiKSkJK3zBQQEYNmyZfj888/h4+MDR0dHPPPMM3j//fexZcsWDBkyBK+99hoCAgI011u+fLkm2SKq03Q0+oxI7z1qGHyrVq3KLX/kyBGhc+fOgomJieDq6ip88MEHwu7duwUAwv79+zXlHjUMfv78+WXOiYeGlD9sypQpAgDhxo0bjywzZ84cAYBw/vx5QRDEIdYff/yx4OnpKRgaGgrOzs7Cyy+/rHWOkpISYf78+ULz5s0FIyMjwcHBQejXr59w+vRpTZm8vDzh9ddfF6ysrAQLCwvhlVdeEVJSUh45DD41NbVMbAkJCcKLL74oWFtbC1ZWVsKQIUOExMTEcu/71q1bQlhYmODg4CDI5XLBy8tLmDRpktYQ7lKtWrUSpFKpkJCQ8MjPpTxKpVIwMTERAAhr164ts//zzz8XOnXqJFhbWwsmJiZC8+bNhS+++EIoKip67HnLGwZvZGT0yOO3b98u+Pn5CcbGxoKHh4fw9ddfa6YwiIuL05RTKBRC//79BQsLCwGA1pD49PR0YfLkyYKbm5tgZGQkNGrUSBg9erSQlpamFdPmzZu1rv24KQ6IapNEEFhfSURUGe3atYOtrS0iIiJ0HQoRVRH7ABERVcKpU6dw7tw5hIWF6ToUInoKrAEiIqqAS5cu4fTp0/juu++QlpaG2NhYGBsb6zosIqoi1gAREVXAli1bMHbsWBQXF2PDhg1MfojqOdYAERERkd5hDRARERHpHSZAREREpHc4EWI51Go1EhMTYWFh8VSrUxMREVHtEQQB2dnZcHV1feIafUyAypGYmAh3d3ddh0FERERVcPv2bTRq1OixZZgAlcPCwgKA+AFaWlrqOBoiIiKqCKVSCXd3d833+OMwASpHabOXpaUlEyAiIqJ6piLdV9gJmoiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7TICIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcPFUImIiBoyQQDy7wKqYkBuDhiaAhVYLLShYwJERERUk1TFQEYckB4DSGWAkTmgKgKKcgB1iVhGZgTILQBIgMJsQFX40DlKgKJsoDBH3F/6KsoBIACCGijKu3dOlbituEAsk5sClBTcP5dMDli6AOZO4jVlRmI5dQlg7ghYugEW9/aXFADFeYCBsVhWKhOvlZcBKBPF68gtxHM+TkkBkJ0kXsepFeDWAXD1B4zMqu1jriwmQERERI+jKgEybwGpV4HUaPHL3MJZTAqUieIXuzIJyEkWv+CL88UvdgM5kJsKZCsAQaXruwAgASCIydXdm+JLl/xHAoOW6OzyTIB0Ie068OuLQOCbQJfJuo6GiKjhUauB4tx7NSU5QP69GouclHtJSq7YNCSRAGaOYo2IsbWYuGQliIlOWrT4Z3qMWGPzNIzMATtvQCIVry8zErfJjAAIQEkhUKgUYzK2FJMrPNBMJZUCckvxGLnFvZe5+F4ivX8NuTkgvffVXlprY2on1ugYyMUaorx0MWHLTb1f2yS3FGt3spMB5R3xs8pNBQxNxCazkkKxBkqtFs9tYg1YuorXKlSKtVyPIzUUk0ZDYyDpPJBwGmgU8HSf6VNiAqQLl/8Asm4D/84GfIIBx+a6joiIGrLCHEBmKH4B6kJxvviFqi4Rv5ANjMXtqmLxC7gkX3wvkwM2TbTjzE4Gki+JZSWS+1/mhiZiU0zaNSDhJJBwCrhzWvzSLj03hOq7BwMTwL4p4OArJhrZSWJNUGlzkaWr+AUvv5e8FOeKTVBmDvf31YV+N6XJk42HriO511SnO0yAdCHxnPinugT4530gbHvd+I9BRLqhVonNEVLZ/d/EAcDIQvzNv1RhNhC1E8hRABauAASxhiIvTSxbWisgMxKbXe7eFJOCzFvi8TIjsV+HpStg6yV+mVu4iF/o5k7il6KZ/f2fRwVZYo21MlFMYux8xGNzU8RaBENT8VWcL9YOCPf6ouSmAll3xN/075wW460oiQywchNjLcoDshOf7rOVyMTPxdhKjN3cSfy7kZlYc6IuEZuushXi/RblAObOgEMzwKE5YO8r/t2qsfazoKdX+u9cR5gA6ULSuft/jzsk1gi1fkln4RBRLSjOFxOC5Ev3mmKSgQKlmEgknb/XmfUhhqaAfTPA1FZMkhJOiTULVaUqEmufs24Dt0+UX8bIXEyEVEVi7Up1MTS939lW0x9GIiZ8hveae4pyxUQqM/6BAyVizYuR+b3kKk2sfSk9h4UL4BYANOogdqy18RATOKnh/aYk/oJJ5WACVNtyUsX2VUiAoEnAscXA8aVMgIjqu5JCIPM2cDdOrHnJjL/XOTZR/D+flXB/xE95Svt8lDYHAeLomwd/YQLEWhi3gHsda9VicmDhci95uDdCqKTgXk2PG+DSVnxJpGINR06KmACl3xATnLw0MRHLThLjLMoRk7RSFq6AVSOxWSrtupi4mTmIr5J88bqGpmKCIpWK92BqJ/apcWghJiYOvmL/Gonkfi0RIMb0YHIiCOJ9Zd4Sy0gNxFoYY0vtz0CthqZ5S8e1CFR/MQGqbUnnxT/tfIB2I8UEKPXa/c54RFS7hHtfpFX5/5eVAFz4TXylXsUT+5yYOwEu/oC1u9gnxNhabI5xai0mCVKZOOJIUIuvrNtiE1dp7VBp8lPVnxUm1mIfG/eO5e8vLhATt7txACSAW3uxSexBavXTNQVJJGKz1KP2WbqIr8dhUxRVAyZAtS3prPinqz9g4wlxzocssVrX3EGXkRFVTUnRvVEkFhU/pjBbnBcl67bYL8O5rfaXWuloHUtXwNRe3KcqFmsmUq+Jo05K50EpKQSsG4u1FIlngFtH741uyRH/T9n7ik0shTliU5JbgFjborgAKC6Kr4IswNZb7Oth7yv2j5EZirEY3RtVU1q+pBBQFwPJVwBlgvZ9GZqJTTC2nmJMlq73Osi63X//pORF9sCPZfum4qu2GBrf6/vS7NFlmHxQA6HzBGjJkiWYP38+FAoF2rZtix9++AGdOnV6ZPmFCxdi2bJliI+Ph729PV5++WXMmzcPxsbGVT5nrSqtAXJpK/6wsXIHsuLFYZZMgKgirmwDDn4DOPsB3d4Vf0NPPCvWZFi6iomI5F7fioebDipDrRabR7KT7k2K5ihuz4wXO/KnRQO3I4GbR8SmmsZBQNNgsWmkQAlc/Qu4c0r88rfzFpt/CrLEZqK8NO1rmdiK/ycsXMTz3jn9wE7J/YnjHp4c7klyFGLS8qBTq8ovmxolvipFAjTuDPiPAJr2FT8j1uQS1Qs6TYA2bdqEadOmYfny5QgMDMTChQsREhKC6OhoODo6lim/fv16zJgxA6tWrUKXLl1w7do1jBkzBhKJBAsWLKjSOWtdYmkC5C/+aed9PwFqEqSzsKiOEwQxyfnvO+DqTnFb8iXg/AY8ttnFwkVsXvHqKfbLuPqX2FRj4yl+WSddEBMOE1vxfemw5MLse6N61PfP5dRa7O9xN678a8UfFV8PuxtX/jGmdmKtTXqsOEdL7P4HdkrERCo3Vby/omxxs7GVmPiZ2IgJXumcJ3dvAndvAY4tAK9eYg2MoamYvKVGix1mjczFJqs7p8X7dG4jvlz8xFjSYsTPJi1aTPJK+6oU5YhNQw7NANf2YgwSiVhL5NqucjVfRFRnSARBeEKjdc0JDAxEx44dsXjxYgCAWq2Gu7s7pkyZghkzZpQpP3nyZERFRSEiIkKz7b333sOJEydw+PDhKp2zPEqlElZWVsjKyoKl5VP8Bv2wvAzgG0/x7zPixR+kf00HTq4Qf5MPnlN916InS74C/POBWCth6Sp+mbUYIHYgjdopdhAtzBa/7CxcxQTB2FJ8bhauYn+KnBQxeY07KCYTjToC7UYAPs+KX84PK8wWR/2VFAK+/cQv6Wu7xCYfF3/xS7a44P5In/y74kjB6F33ayekBkDniWLiEP2XuM3WW5wXRXlHTFIEQWymeWr3OrQ+WGMjkYmJg0Nz8U+vXuLnErVTbIIqvDcc2vsZcV9uipiglE7KZukmNhOV1k6pisXkLj1G/BxM7QDf5wALJ3FfXoaYhEgkgLUHm2CI6JEq8/2tsxqgoqIinD59GjNnztRsk0qlCA4OxrFjx8o9pkuXLli7di0iIyPRqVMnxMbG4u+//8aoUaOqfM5aVTqaw9ZL/BIFxE6NgPjDv764exOI+FTsS9Hh9XtDWGuAIIjDfqO2iXOcePcWfwOXlfPPtqQI2P85EH8c6P4e0CxEPL4wW/zizUsDjiwSa0Bc/MQv5ohPxb4kgNi/49ou4MC8p4s5Zq/4ksjEfl5NugCNu4hxxB8DLv1+/5p/TxfLVXSKfANjoHl/oNs0wLm1uC1bIQ4tNrUtWz4/U0zibkeKtSt56eLEm407izUcOSmAY0txXZ5CpfjeQH5vLpl7M86a2Yt9YXLTgJuHxf1NupbftBb01iMCbw549nj0fckMAfdO4qu8fRZOAJye8OEQEVWOzhKgtLQ0qFQqODlp/2BzcnLC1atXyz3m1VdfRVpaGrp16wZBEFBSUoI333wTH330UZXPCQCFhYUoLLzft0CpVFb1th6vdAJEl7b3t9l5i3+m36iZa1a3zNtA+ACx2e7S78DxZcCAheIXa3W4eRjYO1us+SjOE5swSh34UuygOmS1+KWdnwlkxIpl939xv9/I+lfE2pzSuVYelhV/vxmpSVegy9tiZ9yYCODGPrGGpVnfe1/0VuL8K9mJ4hQGhdni9bITxT/NncRmnMZBYm3I9T3Axc1ignHntPg6+oP29e18xI69t0+IyY9Ta3Gul6Rz4nFG5vcWCJTcTw68eos1RibW2ueycH70Z2lifT+xeNolV8zsgVaDnu4cRER1iM47QVfGgQMH8OWXX2Lp0qUIDAxETEwMpk6dis8++wyffPJJlc87b948zJ07txojfQT7pmITi1fv+9tKE6CM2KcfXlrTlInAL8+LCYR1E7FTa9Zt4LcxwOSTTx66+jiCAESuAHbN0K4RMTARPzNVIXDjgNg/Y0UfcW6R+GPa86oYWwMtXwDOrRebVB7WuAvQabyYZF3aIp63/4L70+53Gi82TUECGBhV7T4adwb6zBITmVvHgFtHxBoYuYUYs08fwOsZ8TnnpIrNbdbuVbsWERFVmc76ABUVFcHU1BRbtmzBoEGDNNtHjx6NzMxMbNu2rcwx3bt3R+fOnTF//nzNtrVr12LChAnIyclBSUlJpc8JlF8D5O7uXv19gMqjKgG+cBb7a7xzqe5+GWYrgNXPARk3xP4bY/4Wm13C+4u1HK1eAgYtA06vFmtd7H3FeU3sm5XfF+ZBeRnAzneBK3+K79u8AnR4Tezz4djyfnNLbhqwdQJw434fMFi4iLU09k2BkC/FocZpMWLi4eAr1hSpS8RanIfnMyEiogalXvQBMjIyQkBAACIiIjTJilqtRkREBCZPLr+6Pi8vD9KHakhkMnFCLUEQqnROAJDL5ZDLdbRIoMxAHLGSdk3sB1QXE6CcVOCXF8Tkx6oxMHqHuFYPADz/f8BPvYDLW8UFCbNulz3e1F5MYsydAc/uYgJ184g4isnQVDxvTrLY9BQ8BwiaXP5QYjN7YMQW4Ey42BTl2x+w9ylbzt6n/O1ERET36LQJbNq0aRg9ejQ6dOiATp06YeHChcjNzcXYsWMBAGFhYXBzc8O8eWLH1AEDBmDBggVo166dpgnsk08+wYABAzSJ0JPOWSfZet9PgLx7P7l8bSrOBzYME5ueLN2A0dvFWpZSLm2BwInA8SVi8mPuJPZVSb8hDinOTRU7IOelic185Q2TBgC7psBLP4kzzz6OVCrWDhERET0FnSZAQ4cORWpqKmbNmgWFQgF/f3/s2rVL04k5Pj5eq8bnf//7HyQSCf73v//hzp07cHBwwIABA/DFF19U+Jx10oP9gOoSQQC2TRInszO2BsK2ibVVD+s9U+ysbOYg/t3E5v6+vAyx71BRjpjk3dgvzsXSOBBwDxSbpiQScdi4kWmt3RoREek3nc4DVFfV2DxAj3JqNbDzHXEm2RGba/56AJCbDhz6RuwfEzJP7PSblyEuT+DWXkxKIj4D/vtWbJoa9afYfEVERFRH1Ys+QPSA0rV+Yg+Kc9V0fqv8uW4eRxCAlChx0rnifHGota23uFbR7UixM7BjC7HcuXXAnk/E2XcBcbbbrlPFTs7KO4DfULED8X/fivuf/z8mP0RE1KCwBqgctV4DpFYBG4YD13eL7337A8PXV+4cpbVIDzI0FefSAQBIgIDRQNp1cYQUICZJ6TcACGITV0Fm2fP2mSVOLEhERFTHVeb7uw5POqNHpDLg1U3AC4sBSMTlDXJSKneOi1vEP63cxbWSDIzF5EciAxxaABCA0+Fi8mNoCjz7GfDWceCZ/4nHFWSKNUav/CoOLQfE0VjdplXPPRIREdUhbAKrKyQSoP0o4MRycXh4/HFxUr+KyM8UJwUExCHqtp7iGkrpMeIaV8ZW4uR/+z4XR2n1/ez+SK7u74nJj+ISMHCJOLzds4dYU9SoA1e2JiKiBokJUF3j3klMgG6fqHgCdGOfOHuyfbP7o7RkhmKfn1Ie3YDXdpU9ViIB+n6uvc3EGnDvWKXwiYiI6gM2gdU17p3FP2+feHy52yeBHVPFIebX7vUdahZSs7ERERE1EKwBqmsaB4p/Jp4TR3MZmpRfbv8X4grfiWfFeXUAoCkTICIioopgDVBdY91E7KejLi5/QU9AHMquuCj+Pek8kJcOyK3EhTiJiIjoiZgA1TUSiThDMiB2hC5PToq4tAQk4igvAPB5Ruz3Q0RERE/EBKguKq3JuR1Z/v7ke7U/dj7AgO/FBUo7Taid2IiIiBoA9gGqi0prgG6fEJu7Hh6Krrgk/unUShw6335U7cZHRERUz7EGqC5y9gNkcnGpirtxZfcnX75XrnXtxkVERNRAMAGqiwyMxNodQBwN9rDk0hqgNrUWEhERUUPCBKiucmkr/pl0Xnt7SSGQdk38e2mSRERERJXCBKiucvUX/0w6p709NRpQl4jLW1g1qu2oiIiIGgR2gq6rHqwBEgQg4SSQcAqQ3ntkTm24ThcREVEVMQGqqxxbAlJDIP+uuKjp+qFip+hS7ABNRERUZWwCq6sM5PcXM434VDv5Adj/h4iI6CmwBqguc/UHFBeAqO3i+65TAYkUuHMa8O2v09CIiIjqMyZAdZmLP4A14t8lUnG2Z3Z8JiIiempsAqvLXPzv/90nmMkPERFRNWECVJc5tbo/6qv9aN3GQkRE1ICwCawuMzQG+n4hLofRLFTX0RARETUYTIDqus5v6joCIiKiBodNYERERKR3mAARERGR3mECRERERHqHCRARERHpHSZAREREpHeYABEREZHeYQJEREREeocJEBEREekdJkBERESkd5gAERERkd5hAkRERER6hwkQERER6R0mQERERKR3mAARERGR3mECRERERHqHCRARERHpHSZAREREpHeYABEREZHeYQJEREREeocJEBEREekdJkBERESkd5gAERERkd6pEwnQkiVL4OHhAWNjYwQGBiIyMvKRZXv16gWJRFLm1b9/f02ZMWPGlNkfGhpaG7dCRERE9YCBrgPYtGkTpk2bhuXLlyMwMBALFy5ESEgIoqOj4ejoWKb81q1bUVRUpHmfnp6Otm3bYsiQIVrlQkNDsXr1as17uVxeczdBRERE9YrOa4AWLFiA8ePHY+zYsWjZsiWWL18OU1NTrFq1qtzytra2cHZ21rz27t0LU1PTMgmQXC7XKmdjY1Mbt0NERET1gE4ToKKiIpw+fRrBwcGabVKpFMHBwTh27FiFzrFy5UoMGzYMZmZmWtsPHDgAR0dH+Pr6YuLEiUhPT3/kOQoLC6FUKrVeRERE1HDpNAFKS0uDSqWCk5OT1nYnJycoFIonHh8ZGYlLly5h3LhxWttDQ0OxZs0aRERE4Ouvv8bBgwfRr18/qFSqcs8zb948WFlZaV7u7u5VvykiIiKq83TeB+hprFy5Em3atEGnTp20tg8bNkzz9zZt2sDPzw/e3t44cOAA+vTpU+Y8M2fOxLRp0zTvlUolkyAiIqIGTKc1QPb29pDJZEhOTtbanpycDGdn58cem5ubi40bN+L1119/4nW8vLxgb2+PmJiYcvfL5XJYWlpqvYiIiKjh0mkCZGRkhICAAERERGi2qdVqREREICgo6LHHbt68GYWFhRg5cuQTr5OQkID09HS4uLg8dcxERERU/+l8FNi0adOwYsUK/PLLL4iKisLEiRORm5uLsWPHAgDCwsIwc+bMMsetXLkSgwYNgp2dndb2nJwcvP/++zh+/Dhu3ryJiIgIDBw4ED4+PggJCamVeyIiIqK6Ted9gIYOHYrU1FTMmjULCoUC/v7+2LVrl6ZjdHx8PKRS7TwtOjoahw8fxp49e8qcTyaT4cKFC/jll1+QmZkJV1dX9O3bF5999hnnAiIiIiIAgEQQBEHXQdQ1SqUSVlZWyMrKYn8gIiKieqIy3986bwIjIiIiqm1MgIiIiEjvMAEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7TICIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7TICIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7TICIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9U+kEqGfPnlizZg3y8/NrIh4iIiKiGlfpBKhdu3aYPn06nJ2dMX78eBw/frwm4iIiIiKqMZVOgBYuXIjExESsXr0aKSkp6NGjB1q2bIlvv/0WycnJVQpiyZIl8PDwgLGxMQIDAxEZGfnIsr169YJEIinz6t+/v6aMIAiYNWsWXFxcYGJiguDgYFy/fr1KsREREVHDU6U+QAYGBnjppZewbds2JCQk4NVXX8Unn3wCd3d3DBo0CPv27avwuTZt2oRp06Zh9uzZOHPmDNq2bYuQkBCkpKSUW37r1q1ISkrSvC5dugSZTIYhQ4ZoynzzzTdYtGgRli9fjhMnTsDMzAwhISEoKCioyu0SERFRA/NUnaAjIyMxe/ZsfPfdd3B0dMTMmTNhb2+P559/HtOnT6/QORYsWIDx48dj7NixaNmyJZYvXw5TU1OsWrWq3PK2trZwdnbWvPbu3QtTU1NNAiQIAhYuXIj//e9/GDhwIPz8/LBmzRokJibizz//fJrbJSIiogai0glQSkoKvvvuO7Ru3Rrdu3dHamoqNmzYgJs3b2Lu3Ln4+eefsWfPHixfvvyJ5yoqKsLp06cRHBx8PyCpFMHBwTh27FiF4lm5ciWGDRsGMzMzAEBcXBwUCoXWOa2srBAYGPjIcxYWFkKpVGq9iIiIqOEyqOwBjRo1gre3N1577TWMGTMGDg4OZcr4+fmhY8eOTzxXWloaVCoVnJyctLY7OTnh6tWrTzw+MjISly5dwsqVKzXbFAqF5hwPn7N038PmzZuHuXPnPvF6RERE1DBUOgGKiIhA9+7dH1vG0tIS+/fvr3JQFbVy5Uq0adMGnTp1eqrzzJw5E9OmTdO8VyqVcHd3f9rwiIiIqI6qdBNYo0aNyh1Rdf36ddy8ebNS57K3t4dMJiszeiw5ORnOzs6PPTY3NxcbN27E66+/rrW99LjKnFMul8PS0lLrRURERA1XpROgMWPG4OjRo2W2nzhxAmPGjKnUuYyMjBAQEICIiAjNNrVajYiICAQFBT322M2bN6OwsBAjR47U2u7p6QlnZ2etcyqVSpw4ceKJ5yQiIiL9UOkE6OzZs+jatWuZ7Z07d8a5c+cqHcC0adOwYsUK/PLLL4iKisLEiRORm5uLsWPHAgDCwsIwc+bMMsetXLkSgwYNgp2dndZ2iUSCd955B59//jm2b9+OixcvIiwsDK6urhg0aFCl4yMiIqLqVVSiRn6RSqcxVLoPkEQiQXZ2dpntWVlZUKkqfzNDhw5FamoqZs2aBYVCAX9/f+zatUvTiTk+Ph5SqXaeFh0djcOHD2PPnj3lnvODDz5Abm4uJkyYgMzMTHTr1g27du2CsbFxpeMjIiKiqhEEATdScxEZl4Fzt+8iKikbSVkFSMspxPshvpjU20dnsUkEQRAqc8CAAQNgYmKCDRs2QCaTAQBUKhWGDh2K3Nxc/PPPPzUSaG1SKpWwsrJCVlYW+wMRERFVQmJmPo7EpOHojXQcvZGGZGVhueVGBzXB3IGtq/Xalfn+rnQN0Ndff40ePXrA19dXMxrsv//+g1KprNQM0ERERFT/ZeQW4diNdBy5kYajMWm4mZ6ntd/IQIr2ja0R0MQGbdys0cjGBC5WxrA1M9JRxKJKJ0AtW7bEhQsXsHjxYpw/fx4mJiYICwvD5MmTYWtrWxMxEhERUR2RU1iCk3EZOBKThiM30hGVpD15sEwqgV8jK3TxtkNXb3u0b2IDY0OZjqJ9tEo3gekDNoERERHddzMtFxFXU7DvajIi4zJQrNJOHZo7W6CLtz26eNuhk5ctLI0NdRJnjTaBlcrLy0N8fDyKioq0tvv5+VX1lERERFQHJGXlIzIuAyfiMnD8Rjpi03K19je2NUVXHzt08bZHkLcd7M3lOoq06iqdAKWmpmLs2LGP7OxclZFgREREpBuCIOBWep4m4Ym8mY7bGflaZQykEgR62eKZ5k54prkjPO3NdBRt9al0AvTOO+8gMzMTJ06cQK9evfDHH38gOTkZn3/+Ob777ruaiJGIiIiq2Y3UHKw5ehP/XFIgJVt7pJZUArR2s0InD1t09LRFkLedzpq1akqlE6B9+/Zh27Zt6NChA6RSKZo0aYJnn30WlpaWmDdvHvr3718TcRIREdFTKCxRYeuZOzh6Ix3Xk7NxVXF/Tj8jmRRt3a3QydMWnTztENDEBubyKveSqRcqfXe5ublwdHQEANjY2CA1NRXNmjVDmzZtcObMmWoPkIiIiKomp7AE5+IzcepWBjZExmvNySORAH2aO2Fk58bo7GVXJ0dq1aRKJ0C+vr6Ijo6Gh4cH2rZtix9//BEeHh5Yvnw5XFxcaiJGIiIiqqD8IhV2XkjEXxeTcCQmTWvElouVMYZ3aoxWrpZo5WoFZyv9XSGh0gnQ1KlTkZSUBACYPXs2QkNDsW7dOhgZGSE8PLy64yMiIqInEAQBlxOV2HE+EZtO3UZmXrFmn5u1Cdo1tkaPpg4Y2M4VcgP9qul5lKeeBygvLw9Xr15F48aNYW9vX11x6RTnASIiorpKpRZwJVGJa8nZuJ6Sg5iUHEQlKXEn8/7IrUY2Jnilgzuea+MMH0cLHUZbu2psHqDi4mI0b94cO3fuRIsWLQAApqamaN++fdWjJSIioscSBAHnE7Kw/Vwidl5ILDNqCwCMDaXo1cwRL7Z3Q3ALJ8ikEh1EWn9UKgEyNDREQUFBTcVCRESk99RqAXcy8xFzr3bneko2TsRl4NYDa2xZGBuglaslmjpawMfRHD6O5mjX2BqmRg175FZ1qvQnNWnSJHz99df4+eefYWDAD5qIiKiicgpLcPh6Gm6m50KRVYCs/GJkF5SgRK2GIABpOYWITc1FfnHZSYVNDGV4tqUTBvq7ontTBxgZSHVwBw1HpTOYkydPIiIiAnv27EGbNm1gZqY9G+TWrVurLTgiIqL6Tq0WcPB6KtYdj8eh66koKlE/8RhDmQRe9mLNjrejOVo4W6BHMweYNfC5eWpTpT9Ja2trDB48uCZiISIiahBUagHnbt9FRFQK/rmkQNwDa2l52JnC390aTlbGsDMzgpncAEYysTbHysQQPo7maGxrCgMZa3hqUqUToNWrV9dEHERERPVaiUqN/66nYcf5ROyPTsHdB4aiW8gN8EpHdwzp0Ai+ThaQSNhBWddYl0ZERFRJJSo1opKycfb2XVxLzoYiqxAXEjK1RmdZGhugp68jnmnugL4tndl8VcdU+ml4eno+NnONjY19qoCIiIjqoozcIhy8loKIqBQcvJaK7IKSMmVszYzwQltXhLZ2RkATGxiyGavOqtJq8A8qLi7G2bNnsWvXLrz//vvVFRcREZHOlajU+DcqGb8cvYUTcelQPzB1sKWxAfwb26C1qyVcrU3Q2NYUnb3sODqrnqjSUhjlWbJkCU6dOvXUAREREelaZl4RNp28jTXHbmnNsNzCxRLPNHfAM82d4O9uzckG67GnXgqjVGxsLPz9/aFUKqvjdDrFpTCIiPRTTmEJfjoUi5//i0VekTgXj42pIV4NbIxhHRvD3dZUxxHS49TYUhiPs2XLFtja2lbX6YiIiGpFVn4x9lxW4ND1NBy6loqsfHH0VnNnC7zW1RMv+LvC2JALiDY0lU6A2rVrp9UJWhAEKBQKpKamYunSpdUaHBERUXUSBAH/XU/DvqspKFapkZFbhH1XU1D4wOSEXvZmeD/EF6GtnTlcvQGrdAI0aNAgrfdSqRQODg7o1asXmjdvXl1xERERVZus/GL8cSYBvx6/hRupuWX2N3MyR2grZ3T1sUdAExtOQqgHqq0PUEPCPkBERPWfIAg4dzsT60/EY8eFRBQUi7U85nIDDPR3haOFMQxkEnRvao82blas7WkAarQP0N9//w2ZTIaQkBCt7bt374ZarUa/fv0qe0oiIqJqIwgCIqJS8MP+GJy/nanZ7utkgVcDG+Ol9m6wMDbUXYBUJ1Q6AZoxYwa++uqrMtsFQcCMGTOYABERUa1LuJuH47EZOHYjHcdj0zVD140MpHi+jQtGdG6M9o1tWMtDGpVOgK5fv46WLVuW2d68eXPExMRUS1BERESPU1Cswp4ryfjvWiqOxaYj4W6+1n4zIxlGBXng9W6ecLCQ6yhKqssqnQBZWVkhNjYWHh4eWttjYmJgZmZWXXERERGVEZOSgw2R8dhyOkEzXB0AZFIJ/BpZobOXHTp72aFDExuuvUWPVel/HQMHDsQ777yDP/74A97e3gDE5Oe9997DCy+8UO0BEhGR/jp8PQ2/HLuJ/CIVsguKcT4hS7PPzdoEz7d1QZCXHTp42MKcCQ9VQqX/tXzzzTcIDQ1F8+bN0ahRIwBAQkICunfvjm+//bbaAyQiIv2iVgs4e/suVh6Ow98XFVr7pBLgmeZOGBHYGD2aOXApCqqyKjWBHT16FHv37sX58+dhYmICPz8/9OjRoybiIyIiPZFbWILVR+Kw5tgtpGQXAhATnlGdm6B9E7EDc4cmNnC1NtFxpNQQcB6gcnAeICKi2lNYosL6E/FYsj8GaTlFAAALuQH6tHDEGz290cKFP4epYmp0HqC3334bPj4+ePvtt7W2L168GDExMVi4cGFlT0lERHqoRKXG1jN38H3Edc2w9SZ2png3uBn6tXGG3IDrb1HNqXQNkJubG7Zv346AgACt7WfOnMELL7yAhISEag1QF1gDRERUc9RqAf9cUuC7vdGIvbcshbOlMd7u0xRDOjSCIZehoCqq0Rqg9PR0WFlZldluaWmJtLS0yp6OiIj0hFotYH90Cv7v32u4dEcJALAxNcRbvXwwKqgJV1ynWlXpBMjHxwe7du3C5MmTtbb/888/8PLyqrbAiIioYUi4m4d/Liqw7sQt3EzPAyBOVDiuuxfGdffkshSkE5VOgKZNm4bJkycjNTUVzzzzDAAgIiIC3333Hfv/EBHpuZtpubicqERMSg5iUnNwTZGN6ORszX4LYwMM79QYb/Twgp05Z2gm3al0AvTaa6+hsLAQX3zxBT777DMAgIeHB5YtW4awsLBqD5CIiOo2QRBw8uZdLN4fg0PXUsvsl0iAjh62eKGtK15s58YZmqlOeKph8KmpqTAxMYG5uTkAICMjA7a2ttUWnK6wEzQR0ZOlZhfiz7N3sOV0gqaWRyaVoLWrJZo6WcDH0RxNHc3h18ia63FRrajRTtAPcnBwAADs2bMHP//8M3bs2IH8/PwnHEVERPVVXlEJ9l1NwZ9n72B/dCpUavF3aCMDKQa3d8PEnj5obGeq4yiJnqzKCdCtW7ewatUq/PLLL7h79y769euHNWvWVGdsRERUBxQUq3AgOhU7LyQiIioF+cUqzT5/d2sM6dAIz/u5wsqEnZmp/qhUAlRUVIStW7fi559/xpEjRxAcHIyEhAScPXsWbdq0qakYiYiolqjVAg5dT8VVhdikdU2RjT1XkpFTWKIp425rguf9XDG4vRt8HC10FSrRU6lwAjRlyhRs2LABTZs2xciRI7Fp0ybY2dnB0NAQMhnnbiAiqs+KVWrsOJ+IHw/Gao3aKuVqZYz+fi543s8Vfo2sIJFwEVKq3yo83eayZcvwxhtvYM+ePZg0aRLs7OyqJYAlS5bAw8MDxsbGCAwMRGRk5GPLZ2ZmYtKkSXBxcYFcLkezZs3w999/a/bPmTMHEolE69W8efNqiZWIqKHJKxIXIO01/wCm/XYe0cnZMJcb4IW2rng5oBHGd/fEljeDcPjDZ/Bx/5Zo627N5IcahArXAP36669YtWoVXFxc0L9/f4waNQr9+vV7qotv2rQJ06ZNw/LlyxEYGIiFCxciJCQE0dHRcHR0LFO+qKgIzz77LBwdHbFlyxa4ubnh1q1bsLa21irXqlUr/Pvvv5r3BgYccklE9KBilRo/HYrFz//F4m5eMQDA3twIY7t6YmTnJuzPQw1ehTOD4cOHY/jw4YiLi0N4eDgmTZqEvLw8qNVqXLlyBS1btqz0xRcsWIDx48dj7NixAIDly5fjr7/+wqpVqzBjxowy5VetWoWMjAwcPXoUhobif04PD4+yN2VgAGdn50rHQ0SkD9JzCjFp/Rkcj80AADS2NcWEHl54OaARl6MgvVHpFec8PT0xd+5c3Lx5E2vXrsXgwYMxcuRINGrUqMwK8Y9TVFSE06dPIzg4+H4wUimCg4Nx7Nixco/Zvn07goKCMGnSJDg5OaF169b48ssvoVKptMpdv34drq6u8PLywogRIxAfH//YWAoLC6FUKrVeREQNiSAIuJKoxII90ei/6DCOx2bAzEiG74a0xb73emJkZ67FRfqlym1DEokEISEhCAkJQUZGBtasWYPVq1dX+Pi0tDSoVCo4OTlpbXdycsLVq1fLPSY2Nhb79u3DiBEj8PfffyMmJgZvvfUWiouLMXv2bABAYGAgwsPD4evri6SkJMydOxfdu3fHpUuXYGFR/miFefPmYe7cuRWOnYiovridkYd1J+Lxz6Uk3Lq3DhcAeNqb4adRAWjqxFFcpJ+eaibop5GYmAg3NzccPXoUQUFBmu0ffPABDh48iBMnTpQ5plmzZigoKEBcXJxm5NmCBQswf/58JCUllXudzMxMNGnSBAsWLMDrr79ebpnCwkIUFhZq3iuVSri7u3MmaCKqlwRBwOVEJcKP3sSfZ++g5N5khXIDKXo0c0C/1s4Ibe0MUyP2j6SGpdZmgn4a9vb2kMlkSE5O1tqenJz8yP47Li4uZYbdt2jRAgqFAkVFRTAyMipzjLW1NZo1a4aYmJhHxiKXyyGXc5p2Iqq/VGoBZ+Lv4r/radhzWaGZxwcAuvnYY3inxujl68B1uIju0dn/BCMjIwQEBCAiIgKDBg0CAKjVakRERGDy5MnlHtO1a1esX78earUaUqnYfenatWtwcXEpN/kBgJycHNy4cQOjRo2qkfsgItK1S3ey8OHvF3A58X7/RSMDKfq2dMLr3TzRrrGNDqMjqpt0+qvAtGnTMHr0aHTo0AGdOnXCwoULkZubqxkVFhYWBjc3N8ybNw8AMHHiRCxevBhTp07FlClTcP36dXz55Zdana+nT5+OAQMGoEmTJkhMTMTs2bMhk8kwfPhwndwjEVFNSbibhxWHYrH2RDxUagEWcgP08HVAj6b2CG3lAitTDmUnehSdJkBDhw5FamoqZs2aBYVCAX9/f+zatUvTMTo+Pl5T0wMA7u7u2L17N9599134+fnBzc0NU6dOxYcffqgpk5CQgOHDhyM9PR0ODg7o1q0bjh8/rlm4lYiovotWZOPHgzew/Xyipn9Pfz8XzBnQiquuE1VQlTpBZ2ZmIjIyEikpKVCr1Vr7wsLCqi04XalMJyoiotpy8mYGlh+4gYirKZptXX3s8FYvH3T1sddhZER1Q412gt6xYwdGjBiBnJwcWFpaak2JLpFIGkQCRERUV2QXFOOvC0nYdOo2zsZnAgAkEqBfa2e82dMbfo2sdRofUX1V6QTovffew2uvvYYvv/wSpqamNRETEZHeyy0swcrDcfjpUKxmJXYjmRSDA9wwvrsXvBzMdRwhUf1W6QTozp07ePvtt5n8EBFVoxupObiYkIXLiVm4kqTEhYQsZBeIiY+Xgxle6eCOl9q7wdHCWMeREjUMlU6AQkJCcOrUKXh5edVEPEREeuVyYha+3hWNQ9dSy+xrYmeK9/r64vk2LpBKuQI7UXWqdALUv39/vP/++7hy5QratGmjWZS01AsvvFBtwRERNVTZBcX46p+rWHdCXKvQQCpBu8bWaOliiZaulmjpYoUWLhYwkFV6yUYiqoBKjwJ7cFh6mZNJJGUWJq2POAqMiGpKQbEKf569g+8jriMpqwAAMKCtK6b3bYYmdmY6jo6ofqvRUWAPD3snIqInK1GpsfrITSw5EIPMvGIAYhPXvJfaoIs3h7AT1TYuCkNEVINSlAU4E5+JJftjcPFOFgCgkY0JRgd5YGTnJjAxkj3hDERUEyqUAC1atAgTJkyAsbExFi1a9NiyDy5LQUSkj0pUavxzSYEV/8XiQkKWZrulsQE+7t8CLwe4Q8ZOzUQ6VaE+QJ6enjh16hTs7Ozg6en56JNJJIiNja3WAHWBfYCIqCryikrw28nbWHkkDrcz8gGIkxY2c7RAoJctJvf2gaMlh7ET1ZRq7wMUFxdX7t+JiPSdIAj4NyoFf567g/1XU5BXJA4EsTE1RFiQB8KCmsDOnOtzEdU17ANERFRFVxVKzN52GSfiMjTbmtiZYlw3T7wc4M7+PUR1WJUSoISEBGzfvh3x8fEoKirS2rdgwYJqCYyIqK4SBAFrT8Rj7vbLKFELkBtIMapzEzzf1hVtG1lprZFIRHVTpROgiIgIvPDCC/Dy8sLVq1fRunVr3Lx5E4IgoH379jURIxFRnRGXlotlB2Lw26kEAEBwCyfMeaElGtlweSCi+qTSCdDMmTMxffp0zJ07FxYWFvj999/h6OiIESNGIDQ0tCZiJCLSGUEQEJ2cjX8uKrD7sgJXFdkAxM7NH4Y2xxs9vFjjQ1QPVToBioqKwoYNG8SDDQyQn58Pc3NzfPrppxg4cCAmTpxY7UESEdW2ZGUBwo/exK5LCsSl5Wq2G0glCPK2wxs9vNGtKScwJKqvKp0AmZmZafr9uLi44MaNG2jVqhUAIC0trXqjIyKqZWq1gN9O3cYXf0dpVmM3MpCiR1N7hLZ2QXALR1ibGuk4SiJ6WpVOgDp37ozDhw+jRYsWeO655/Dee+/h4sWL2Lp1Kzp37lwTMRIR1ZicwhIcv5GOs7fv4tztTFy4nYXsQjHx8WtkhXHdvfBMc0eYyzlolqghqfT/6AULFiAnJwcAMHfuXOTk5GDTpk1o2rQpR4ARUb2hUgvYdPI2vtsTjfRc7dGsFnIDTA1uirFdPTljM1EDVakESKVSISEhAX5+fgDE5rDly5fXSGBERDXl2I10fLrzCqKSlAAAN2sTdPWxg7+7DfzdrdHMyRwGMqmOoySimlSpBEgmk6Fv376IioqCtbV1DYVERFQz4tPz8OXfUdh1WQFAXJvrneBmGBXUBIZMeIj0SqWbwFq3bo3Y2NjHrglGRFRXFBSrcOrmXfwblYz1J+JRpFJDKgFGBDbBu882g60ZOzQT6aNKJ0Cff/45pk+fjs8++wwBAQEwMzPT2s/FQ4moLihRqbHuRDwW7L2GrPxizfauPnb45PmWaO7Mn1VE+qxCq8EDwKeffor33nsPFhYW9w9+YPIvQRAgkUigUqmqP8paxtXgieq3E7HpmL39smbSQkcLObo3dcDzfi7o5evAiQuJGqjKfH9XOAGSyWRISkpCVFTUY8v17Nmz4pHWUUyAiOqny4lZ+PFgLLafTwQAWJkYYnqIL17t1JijuYj0QGW+vyvcBFaaJzWEBIeIGo78IhV2XEjEuhPxOH87E4C4TMXwTo0xva8v+/gQUbkq1QeI1cZEVJf8cTYBs7Zd1szYbCiTIKSVM97s6Y3WblY6jo6I6rJKJUDNmjV7YhKUkZHxVAERET2JIAhY+O91fB9xHQDQ2NYUwzs1xpAOjWBvLtdxdERUH1QqAZo7dy6srPhbFRHpTnpOIWZsvYi9V5IBAG/18sb0vr6Qso8PEVVCpRKgYcOGwdHRsaZiISJ6pMISFbafS8TXu6KRllMIQ5kEnw1sjWGdGus6NCKqhyqcALH/DxHVtryiEhyJScfh66n462IS0nLENbuaOppj4TB/tHJljTQRVU2lR4EREdW0lOwC/HL0JtYej9eaxNDFyhijgprgta6eMDaU6TBCIqrvKpwAqdXqmoyDiAixqTn46VAstp65gyKV+DPHzdoEvXwd0LOZA3o3d+SaXURULSq9FAYRUXVTZBVg4b/X8Nup21Dfq2xu39gaE3p449mWTpzEkIiqHRMgItIZlVpA+NGb+HZ3NPKLxWV0+jR3xMRe3ujgYavj6IioIWMCRES16mZaLraeSUBceh6uJilxPSUHgFjj89FzLZj4EFGtYAJERLXiVnouPtt5BRFXU/DgmAoLuQFmPtcCwzq6cy4fIqo1TICIqMbtupSE9zdfQHahuGRFz2YO6OZjDycrYwR52cHBgrM3E1HtYgJERDVGrRbw7Z5oLD1wAwAQ0MQGXw/2g4+juY4jIyJ9xwSIiGqEsqAYH229iJ0XkgAAE3p44f0QXw5jJ6I6gQkQEVWrs/F38X3EdRyJSUOxSoChTIJ5L/nh5YBGug6NiEiDCRARVZvt5xMx/bfzmkkMmzqaY+7AVujiba/jyIiItDEBIqKnplIL+GHfdSz89zoA4NmWTvgw1Bc+jhY6joyIqHxMgIjoqaTlFOKdjedwOCYNADC+uydm9GvB2ZuJqE5jAkREVXb4ehre/e0cUrMLYWIow+eDWmMw+/oQUT2g8+EYS5YsgYeHB4yNjREYGIjIyMjHls/MzMSkSZPg4uICuVyOZs2a4e+//36qcxJR5RSr1Pjqn6sYteoEUrML0dTRHNsnd2XyQ0T1hk4ToE2bNmHatGmYPXs2zpw5g7Zt2yIkJAQpKSnlli8qKsKzzz6LmzdvYsuWLYiOjsaKFSvg5uZW5XMSUeXEp+fh5eXHsPzgDQgC8GpgY2yf3A1Nndjfh4jqD4kgPDgpfe0KDAxEx44dsXjxYgCAWq2Gu7s7pkyZghkzZpQpv3z5csyfPx9Xr16FoaFhtZyzPEqlElZWVsjKyoKlpWUV746oYSksUeHn/+KweF8M8otVsDQ2wDcv+yG0tYuuQyMiAlC572+d1QAVFRXh9OnTCA4Ovh+MVIrg4GAcO3as3GO2b9+OoKAgTJo0CU5OTmjdujW+/PJLqFSqKp+TiB5v39VkvL/5PLp/vR/z763aHuhpi3/e6cHkh4jqLZ11gk5LS4NKpYKTk5PWdicnJ1y9erXcY2JjY7Fv3z6MGDECf//9N2JiYvDWW2+huLgYs2fPrtI5AaCwsBCFhYWa90ql8inujKjh+PHgDcz75/7/HQcLOT5+rgUG+rtCIuEoLyKqv+rVKDC1Wg1HR0f89NNPkMlkCAgIwJ07dzB//nzMnj27yuedN28e5s6dW42REtV/GyPjNcnPsI7ueK6NCzp52sLYUKbjyIiInp7OEiB7e3vIZDIkJydrbU9OToazs3O5x7i4uMDQ0BAy2f0fwC1atIBCoUBRUVGVzgkAM2fOxLRp0zTvlUol3N3dq3JbRPXencx8/N/ea/j9TAIA4I2eXpjZr4WOoyIiql466wNkZGSEgIAAREREaLap1WpEREQgKCio3GO6du2KmJgYqNVqzbZr167BxcUFRkZGVTonAMjlclhaWmq9iPRJUYkam0/dxtjVkeg1fz+2nE6AIABjunhgRmhzXYdHRFTtdNoENm3aNIwePRodOnRAp06dsHDhQuTm5mLs2LEAgLCwMLi5uWHevHkAgIkTJ2Lx4sWYOnUqpkyZguvXr+PLL7/E22+/XeFzEpG22xl5mLzhLM7fztRs6+xliw9Dm6NdYxvdBUZEVIN0mgANHToUqampmDVrFhQKBfz9/bFr1y5NJ+b4+HhIpfcrqdzd3bF79268++678PPzg5ubG6ZOnYoPP/ywwuckIlGJSo0tpxPw5d9RUBaUwMrEEOO6eaJfG2eu4UVEDZ5O5wGqqzgPEDV0kXEZ+OiPi4hJyQEA+LtbY/Gr7dDIxlTHkRERVV1lvr/r1SgwIno6giDg1+O38OmOKyhRC7AxNcSk3j4IC/KAkYHOV8YhIqo1TICI9ISyoBizt13GH2fvAAAG+rvis0GtYWlc/qzqREQNGRMgIj1w+tZdvL3hLO5k5kMqAWb0a47x3b04mSER6S0mQEQN3Nn4uxi18gTyilRwtzXBwqH+CGhiq+uwiIh0igkQUQN2LTkbY8NPIq9IhW4+9lg+KgDmcv63JyLiT0KiBio1uxCjV0UiM68Y/u7W+HFUAMyY/BARAdDhTNBEVHOKStR4a91pJGUVwMvBDOFjOzL5ISJ6ABMgogYmr6gEM7ZewMmbd2EhN8CKsA6wNjXSdVhERHUKfyUkaiCKVWr8fTEJX/9zFYlZBZBIgIXD/OHtYK7r0IiI6hwmQET1XEp2ATZG3sa6E7eQrCwEALhZm2DuC63QpwWXgCEiKg8TIKJ6SBAEnL51F2uO3cI/l5JQrBJXtLE3N0JYkAcm9PCCsaFMx1ESEdVdTICI6pH8IhW2nbuDNcdu4UqSUrO9fWNrjO7igdDWzpAbMPEhInoSJkBE9cCt9Fz8euwWfjt1G8qCEgCA3ECKgf6uCAvyQGs3Kx1HSERUvzABIqpjUrMLseuyAjHJ2biekoOYlBykZBdq9rvbmmBU5yZ4pYM7R3cREVUREyCiOkIQBGw+lYDP/7qiqeUpJZEAPZs5ICyoCXo2c4RMyjW8iIieBhMgojogPj0PH/1xEYdj0gAAzZ0t0LOZA7wdzdHU0RzejuZctZ2IqBoxASLSsd9O3sbs7ZeRX6yC3ECKac82w+vdPGEg4zylREQ1hQkQkQ5tP5+ID36/AAAI9LTFV4P94GlvpuOoiIgaPiZARDpyPDYd0387DwAY08UDs55vCSn79hAR1QrWsRPpwK30XLzx62kUqdTo19oZnzD5ISKqVUyAiGpZTmEJxq85haz8Yvi7W+P/hvpzVBcRUS1jAkRUiwqKVXhn4zlcS86Bo4UcP44K4JIVREQ6wD5ARLVEkVWAN9aexvnbmTCSSbF8VACcLI11HRYRkV5iAkRUw9RqAVvP3sG8v6OQnlsEa1NDLB7eHu0b2+g6NCIivcUEiKgalajUOB6bAbmhFI1sTHD4ehrWHr+F8wlZAMQJDn8a1QGN7Ux1HCkRkX5jAkRUTa4kKvHh7xdw8U5WmX1mRjK83acpxnb1hJEBu94REekaEyCiKsrKK8amU/H4+6ICiZn5SM0phCAAFnIDWBgbIDGrAJ72Zng5oBGGdGgERwv29yEiqiuYABFVwW+nbmP2NnH5igf1a+2MuQNbwdHCGEUlahjKJJBIOMSdiKiuYQJEVEkHr6Vi5taLUKkFNHe2QFiQB9q4WcHV2hh25nJNOTZ1ERHVXUyAiCrg0p0snE/IRFGJGgv2XINKLWBw+0b4dogfa3iIiOohJkBEjyAIAvZdTcHygzdw8uZdrX2dPG0x76U2TH6IiOopJkBE5YhLy8XcHZdxIDoVAGAglSDI2w6mRjK4WJlgap+mbOIiIqrHmAARPeTSnSwM/+k4sgtLYCiT4LWunnitmydnbSYiakCYABE9IDY1B6NXRSK7sATtGlvjuyFt4eVgruuwiIiomjEBIronMTMfo1ZGIj23CK3dLPHLa51gaWyo67CIiKgGsBMDEYCM3CKMWnkCdzLz4eVghl/GMvkhImrIWANEeksQBOy+rMDx2AwcupaK2LRcuFgZ49fXA7Xm8yEiooaHCRDprfCjNzF3xxXNe1szI/z6eiDcrE10GBUREdUGJkCkl64qlJj3z1UAwEvt3RDoaYvezR25XhcRkZ5gAkR6J7ugGFM3nENRiRp9mjviuyFtOaEhEZGeYQJEeiUiKhn/+/MSkrIKYG8ux9cvcykLIiJ9xASIGrSkrHzkF6kQn5GH5Qdv4HhsBgDAw84U3w9rB3t2diYi0ktMgKhBEgQBc3dcQfjRm1rbS2d2fie4GUyMZLoJjoiIdI4JEDVI607Ea5IfC2MDyA2k6N/GBW/09IYrR3kREek9JkDUoIhz+yRjzvbLAIAPQn3xVi8fHUdFRER1TZ2YCXrJkiXw8PCAsbExAgMDERkZ+ciy4eHhkEgkWi9jY+2hy2PGjClTJjQ0tKZvg3Ts9K0MDFpyBG+uPY0StYDn/Vwwsae3rsMiIqI6SOc1QJs2bcK0adOwfPlyBAYGYuHChQgJCUF0dDQcHR3LPcbS0hLR0dGa9+WN4gkNDcXq1as17+VydnZtyM7G38XInyORX6yCsaEUIwKbYHpfX47wIiKicuk8AVqwYAHGjx+PsWPHAgCWL1+Ov/76C6tWrcKMGTPKPUYikcDZ2fmx55XL5U8sQw3DjdQcvBZ+EvnFKnRvao//G+rP0V1ERPRYOm0CKyoqwunTpxEcHKzZJpVKERwcjGPHjj3yuJycHDRp0gTu7u4YOHAgLl++XKbMgQMH4OjoCF9fX0ycOBHp6ek1cg+kW4qsAoStjMTdvGK0bWSF5SMDmPwQEdET6TQBSktLg0qlgpOTk9Z2JycnKBSKco/x9fXFqlWrsG3bNqxduxZqtRpdunRBQkKCpkxoaCjWrFmDiIgIfP311zh48CD69esHlUpV7jkLCwuhVCq1XlT3ZeUXY/SqSNzJzIenvRlWjekIM7nOKzWJiKgeqHffFkFBQQgKCtK879KlC1q0aIEff/wRn332GQBg2LBhmv1t2rSBn58fvL29ceDAAfTp06fMOefNm4e5c+fWfPBUbXIKS/B6+ElEJ2fD0UKONa914gruRERUYTqtAbK3t4dMJkNycrLW9uTk5Ar33zE0NES7du0QExPzyDJeXl6wt7d/ZJmZM2ciKytL87p9+3bFb4JqXVZ+MUatPIFTt+7CwtgA4WM7wd3WVNdhERFRPaLTBMjIyAgBAQGIiIjQbFOr1YiIiNCq5XkclUqFixcvwsXF5ZFlEhISkJ6e/sgycrkclpaWWi+qm+7mFmHEz8dxNj4TViaGWDcuEC1d+byIiKhydD4P0LRp07BixQr88ssviIqKwsSJE5Gbm6sZFRYWFoaZM2dqyn/66afYs2cPYmNjcebMGYwcORK3bt3CuHHjAIgdpN9//30cP34cN2/eREREBAYOHAgfHx+EhITo5B6peqTlFGL4iuO4dEcJWzMjbBjfGX6NrHUdFhER1UM67wM0dOhQpKamYtasWVAoFPD398euXbs0HaPj4+Mhld7P0+7evYvx48dDoVDAxsYGAQEBOHr0KFq2bAkAkMlkuHDhAn755RdkZmbC1dUVffv2xWeffca5gOqxqwolJq07gxupuXCwkGP9uEA0dbLQdVhERFRPSQRBEHQdRF2jVCphZWWFrKysGm8OS1EWwNrUCEYGOq+MqxMEQUBsWi7UagEmRjIkZRXg2I10LN4XgyKVGi5Wxlg3LhBeDua6DpWIGjCVSoXi4mJdh0EPMTQ0hEz26IWsK/P9rfMaIH12LTkboQsPoV9rFywZ0V7X4ehUQbEK288nIvzITVxJKn8agj7NHfHVYD84WLAmj4hqhiAIUCgUyMzM1HUo9AjW1tZwdnZ+6pn+mQDp0FVFNtQCsO9qCopVahjK9K8WSJFVgLXHb2F9ZDwycosAAEYGUpgayZBbWAJHC2N4O5rjeT8XDAloxKUtiKhGlSY/jo6OMDU15c+cOkQQBOTl5SElJQUAHjv4qSKYAOlQVr5YvZpfrMKlO1lo19hGxxHVnptpufh2TzR2XVKgRC22wrpZm2BUUBMM6+gOa1MjHUdIRPpGpVJpkh87Oztdh0PlMDExAQCkpKTA0dHxsc1hT8IESIeU+ffbl0/ezNCbBCgqSYmRP59A+r0an06etnitqweCWzjBQA9rwYiobijt82NqynnF6rLS51NcXMwEqL7KeiABioy7iwk9dBhMDRMEAQl383H61l3M2XEZmXnFaOVqiW9e9kMrVytdh0dEpMFmr7qtup4PEyAdysq7nwCdupUBtVqAVNrw/uOVqNSYvP4sdl2+v76bv7s1fnmtE6xMDHUYGRERVSeJRII//vgDgwYN0nUoT8T2Bh16sAYoM68YMak5Ooym8o7GpGHCmlP47eTjlw75dOcV7LqsgEwqgV8jK0zo4YVfX2fyQ0RUXcaMGQOJRFLmFRoaquvQnmjr1q3o27cv7OzsIJFIcO7cuVq5LmuAdOjBBAgAIuMy0Oze5H6CIEClFupEn5j9V1Pwxd9RyC4ohgQS2JkbwUAqwfmELABAxNUUtHS1RGs3sSlLEARsP5+I87ezkJFbiD/PJUIiAZaOaI+QVhVb442IiConNDQUq1ev1tpWHyYAzs3NRbdu3fDKK69g/PjxtXZd3X+76rHSBKituzUAsSM0AKjUAvovOoznFv2HgmKVrsIDAFy6k4W31p1BTEoOkpWFUCgLcDlRifMJWTCUSeDtYAaVWsD0zedRVKJGVn4xJq49g6kbz2HVkTj8eS4RADAjtDmTHyKiGiSXy+Hs7Kz1srG5P7hGIpFg2bJl6NevH0xMTODl5YUtW7ZonePixYt45plnYGJiAjs7O0yYMAE5OdqtE6tWrUKrVq0gl8vh4uKCyZMna+1PS0vDiy++CFNTUzRt2hTbt29/bNyjRo3CrFmzEBwc/JSfQOWwBkiHShOg4OaOOH87E2fi7wIAEjPzNZMB7jifiCEd3HUSX2p2ISasOYX8YhV6NHPAh6G+UKvFNbnSc4sQ6GkLUyMZnv2/Q7iqyEbo94eQqixEdmEJDGUSDOvYGObGBmjhYokBfk83XwMRkS4IgoB8Hf0iamIoq/YO2Z988gm++uorfP/99/j1118xbNgwXLx4ES1atEBubi5CQkIQFBSEkydPIiUlBePGjcPkyZMRHh4OAFi2bBmmTZuGr776Cv369UNWVhaOHDmidY25c+fim2++wfz58/HDDz9gxIgRuHXrFmxtbav1Xp4WEyAdUhaICVBnbztgL5BwNx8FxSrEpeVqyvxy7CZeroUJAAtLVDhzKxMqtYAu3nYoVqvxxq+nkJhVAC8HM/wwvN0j++x8OrAVJq8/i9hUMW53WxP8MLw9/O/VbBER1Vf5xSq0nLVbJ9e+8mkITI0q/jW9c+dOmJtrLxP00Ucf4aOPPtK8HzJkiGbx8M8++wx79+7FDz/8gKVLl2L9+vUoKCjAmjVrYGZmBgBYvHgxBgwYgK+//hpOTk74/PPP8d5772Hq1Kmac3bs2FHrmmPGjMHw4cMBAF9++SUWLVqEyMjIOtcfiQmQjqjUArILSgAAnvZmsDIxRFZ+MeLScnEz/X4CdOmOEmfiMxHQpObmCFqw9xpWHIrV/JbTs5kDrE0NcSY+E5bGBvg5rMNjOyw/7+cKE0MZcgpL0NTRAk2dzPVyVmsiIl3q3bs3li1bprXt4VqXoKCgMu9LOx1HRUWhbdu2muQHALp27Qq1Wo3o6GhIJBIkJiaiT58+j43Dz89P83czMzNYWlpqZm+uS5gA6Uh2wf0O0FYmhvByMMPZ+EzEpuZqalKkEkAtAGuO3ayxBCgjtwhL9sdApRZgby5HdkExDl5L1Vx/8avtK7TwaJ8WTjUSHxGRLpkYynDl0xCdXbsyzMzM4OPjU0PR3J+F+UkMDbV/YZZIJFCr1TUR0lPhr+k6Utr/x9RIBkOZFF72YpIRm5qjqQEa3qkxAODvi0m4e2/W5Oq2+7ICKrWAli6WOPlxH2yb3BVeDmL2/8nzLdGjmUONXJeIqD6QSCQwNTLQyasmuj4cP368zPsWLVoAAFq0aIHz588jN/d+K8SRI0cglUrh6+sLCwsLeHh4ICIiotrj0gXWAOlIaQJkaSxmyqVJx43UHE0foAFtXREZl4HrKTmIvJlRI6Oo/rqQBAB4vq0LJBIJmjtb4p+p3aHIKkATO7MnHE1ERHVFYWEhFAqF1jYDAwPY29tr3m/evBkdOnRAt27dsG7dOkRGRmLlypUAgBEjRmD27NkYPXo05syZg9TUVEyZMgWjRo2Ck5NYyz9nzhy8+eabcHR0RL9+/ZCdnY0jR45gypQpVY47IyMD8fHxSEwURw1HR0cDgGYkW01hDZCOlCZApX1rvO81M0Un5yDhbj4AsW9QR0+x/fbUvSHy1Sk9pxBHb6QBAJ5v46rZLjeQMfkhIqpndu3aBRcXF61Xt27dtMrMnTsXGzduhJ+fH9asWYMNGzagZcuWAMQ1tnbv3o2MjAx07NgRL7/8Mvr06YPFixdrjh89ejQWLlyIpUuXolWrVnj++edx/fr1p4p7+/btaNeuHfr37w8AGDZsGNq1a4fly5c/1XmfhDVAOlI2ARITjqh7w99NjWRwtJCjo4cN1p+Ix8mbd6s9hn8uKaAWAL9GVmhsx8X/iIjqq/DwcM1Q9cdxdXXFnj17Hrm/TZs22Ldv32PP8cYbb+CNN94od58gCGW2ZWZmPvZ8Y8aMwZgxYx5bpiawBkhHNE1g9xKgxnameHAZMA87M0gkEnRoItYAXbqThfyi6p2LorT5q38bztFDRET6hQmQjjxcAyQ3kKGx7f1aGE97sUaokY0JXKyMUaIWcPZ29dUCXUvOxvG4dADAc0yAiIhIzzAB0pGHEyAAWsPNSxMgiUSCDh6l/YCqLwFavC8GggD0a+0Md1s2fxERNXSCINSLVdprCxMgHVGWlwDZ3+947PHA3zt6iHMAnXzKjtDKgmLkFpYgNjUHOy+Ive0nP1Nzc0YQERHVVewErSPKfHEWaCuT+49Auwbofq1MaT+gM7fuokSlrtIK8cnKAoQsPISCYhWcLY2hFoDgFo5o5WpV1VsgIiKqt1gDpCOaJjDT+zVApSPBAMDT/n4y5OtsAQu5AXKLVLh4J6tK19sQGY/MvGIUFKtxMz0PADDlmaZVOhcREVF9xxogHSmvD1BzF0uYGcngYCGHzQOJkUwqQe/mjth+PhG/n0lAu8aVWxajRKXGppO3AQBv92mKnIISuNuaoC0XKyUiIj3FBEhHykuArEwM8e97PWFiKCszBfrQju7Yfj4R284l4uPnWsLEqOJrxByITkVSVgFszYwwqbc35AaVW1+GiIiooWECpCPlJUAA4GJV/mJzQV52aGRjgoS7+fjnUhL6+7ng2I10cQHVtFzYmRmhsa0phnRoBIt7y2vkFZWgoFiNtSduAQBeDmjE5IeIiAhMgHRCrRagLNBeC+xJpFIJhgS44//+vYafDsVi8f4YzarxD9p1WYH14wKxPzoV72w8i9wHJk8sXVyViIioJkgkEvzxxx/1Yrg9O0HrQHZhCUpnC7c0qVgCBAAvd2gEiQS4qshGbKpY6/NSOzfM6Nccb/XyhrncAJFxGXh/ywW8vUE7+env56KZW4iIiBqWMWPGQCKRlHmFhobqOrTHKi4uxocffog2bdrAzMwMrq6uCAsL0yyMWpNYA6QDpXMAyQ2kMDaseJOUm7UJXmjrim3nEjG0gzs+eq6F1iiy5i6WeHvDWfxx9g4AoHtTe6wI64BilRrmcj5qIqKGLDQ0FKtXr9baJpfLdRRNxeTl5eHMmTP45JNP0LZtW9y9exdTp07FCy+8gFOnTtXotVkDpAOP6v9TEd8OaYvT/wvG1y/7aSU/APBCW1dNM5e3gxkWv9oexoYyWBgblulUTUREDYtcLoezs7PWy8bm/qhhiUSCZcuWoV+/fjAxMYGXlxe2bNmidY6LFy/imWeegYmJCezs7DBhwgTk5ORolVm1ahVatWoFuVwOFxcXTJ48WWt/WloaXnzxRZiamqJp06bYvn37I2O2srLC3r178corr8DX1xedO3fG4sWLcfr0acTHx1fDp/JoTIB04GkSIEOZFHbmj87oPx3YCj+OCsDvE7tU6fxERPQAQQCKcnXzKmdl9af1ySefYPDgwTh//jxGjBiBYcOGISoqCgCQm5uLkJAQ2NjY4OTJk9i8eTP+/fdfrQRn2bJlmDRpEiZMmICLFy9i+/bt8PHRXlFg7ty5eOWVV3DhwgU899xzGDFiBDIyKr6SQVZWFiQSCaytravlnh+F7SI68DQJ0JMYyqQIaeVc7eclItJLxXnAl666ufZHiYBRxftu7ty5E+bm5lrbPvroI3z00Uea90OGDMG4ceMAAJ999hn27t2LH374AUuXLsX69etRUFCANWvWwMxMvO7ixYsxYMAAfP3113BycsLnn3+O9957D1OnTtWcs2PHjlrXHDNmDIYPHw4A+PLLL7Fo0SJERkZWqD9SQUEBPvzwQwwfPhyWlpYVvveqYAKkA/9dTwMAOFjU7bZZIiKqP3r37o1ly5ZpbbO1tdV6HxQUVOb9uXPnAABRUVFo27atJvkBgK5du0KtViM6OhoSiQSJiYno06fPY+Pw8/PT/N3MzAyWlpZISUl5YvzFxcV45ZVXIAhCmfuoCUyAatmFhExsPCm2a47p4qHbYIiI6PEMTcWaGF1duxLMzMzKNEdVJxOT8uepe5ihoXbrhkQigVqtfuwxpcnPrVu3sG/fvhqv/QHYB6hWqdUCPtl2GYIADPJ3RaCXna5DIiKix5FIxGYoXbxqYPDK8ePHy7xv0aIFAKBFixY4f/48cnPvzzF35MgRSKVS+Pr6wsLCAh4eHoiIiKjWmEqTn+vXr+Pff/+FnV3tfDeyBqgW/XbqNs7fzoS53AAfPddC1+EQEVEDUlhYCIVCobXNwMAA9vb2mvebN29Ghw4d0K1bN6xbtw6RkZFYuXIlAGDEiBGYPXs2Ro8ejTlz5iA1NRVTpkzBqFGj4OTkBACYM2cO3nzzTTg6OqJfv37Izs7GkSNHMGXKlCrFXFxcjJdffhlnzpzBzp07oVKpNPdga2sLIyOjKp23IpgA1aLcIhXkBlK8E9wUjpbGug6HiIgakF27dsHFxUVrm6+vL65evap5P3fuXGzcuBFvvfUWXFxcsGHDBrRs2RIAYGpqit27d2Pq1Kno2LEjTE1NMXjwYCxYsEBz/OjRo1FQUID/+7//w/Tp02Fvb4+XX365yjHfuXNHM0ze399fa9/+/fvRq1evKp/7SSSCUAPj7Oo5pVIJKysrZGVlVXs75O2MPDhbGcNQxtZHIqK6pKCgAHFxcfD09ISxccP7JbU+LVPxOI97TpX5/mYNUC1zt61cpzYiIiKqfqyGICIiIr3DGiAiIiI9wB4v2lgDRERERHqHCRARERHpHSZARERED2BTUd1WXc+HCRARERHuL+GQl5en40jocUqfz8NLblQWO0ETEREBkMlksLa21izcaWpqCkkNLEdBVSMIAvLy8pCSkgJra2vIZLKnOl+dSICWLFmC+fPnQ6FQoG3btvjhhx/QqVOncsuGh4dj7NixWtvkcjkKCgo07wVBwOzZs7FixQpkZmaia9euWLZsGZo2bVqj90FERPWbs7MzAFRo9XLSDWtra81zeho6T4A2bdqEadOmYfny5QgMDMTChQsREhKC6OhoODo6lnuMpaUloqOjNe8fztC/+eYbLFq0CL/88gs8PT3xySefICQkBFeuXGmQs3sSEVH1kEgkcHFxgaOjI4qLi3UdDj3E0NDwqWt+Sul8KYzAwEB07NgRixcvBgCo1Wq4u7tjypQpmDFjRpny4eHheOedd5CZmVnu+QRBgKurK9577z1Mnz4dAJCVlQUnJyeEh4dj2LBhT4ypJpfCICIioppRme9vnXaCLioqwunTpxEcHKzZJpVKERwcjGPHjj3yuJycHDRp0gTu7u4YOHAgLl++rNkXFxcHhUKhdU4rKysEBgY+8pyFhYVQKpVaLyIiImq4dJoApaWlQaVSwcnJSWu7k5MTFApFucf4+vpi1apV2LZtG9auXQu1Wo0uXbogISEBADTHVeac8+bNg5WVlebl7u7+tLdGREREdVi9GwYfFBSEsLAw+Pv7o2fPnti6dSscHBzw448/VvmcM2fORFZWluZ1+/btaoyYiIiI6hqddoK2t7eHTCZDcnKy1vbk5OQK9/A2NDREu3btEBMTA+B+D/7k5GS4uLhondPf37/cc8jlcsjlcs370m5RbAojIiKqP0q/tyvSvVmnCZCRkRECAgIQERGBQYMGARA7QUdERGDy5MkVOodKpcLFixfx3HPPAQA8PT3h7OyMiIgITcKjVCpx4sQJTJw4sULnzM7OBgA2hREREdVD2dnZsLKyemwZnQ+DnzZtGkaPHo0OHTqgU6dOWLhwIXJzczVz/YSFhcHNzQ3z5s0DAHz66afo3LkzfHx8kJmZifnz5+PWrVsYN24cAHEI4zvvvIPPP/8cTZs21QyDd3V11SRZT+Lq6orbt2/DwsKiWibBUiqVcHd3x+3btzmqrI7iM6rb+HzqPj6juk1fno8gCMjOzoarq+sTy+o8ARo6dChSU1Mxa9YsKBQK+Pv7Y9euXZpOzPHx8ZBK73dVunv3LsaPHw+FQgEbGxsEBATg6NGjaNmypabMBx98gNzcXEyYMAGZmZno1q0bdu3aVeE5gKRSKRo1alS9Nwpx/qKG/A+vIeAzqtv4fOo+PqO6TR+ez5NqfkrpfB4gfcB5heo+PqO6jc+n7uMzqtv4fMqqd6PAiIiIiJ4WE6BaIJfLMXv2bK2RZlS38BnVbXw+dR+fUd3G51MWm8CIiIhI77AGiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSoFixZsgQeHh4wNjZGYGAgIiMjdR2SXpgzZw4kEonWq3nz5pr9BQUFmDRpEuzs7GBubo7BgweXWZcuPj4e/fv3h6mpKRwdHfH++++jpKSktm+lQTh06BAGDBgAV1dXSCQS/Pnnn1r7BUHArFmz4OLiAhMTEwQHB+P69etaZTIyMjBixAhYWlrC2toar7/+OnJycrTKXLhwAd27d4exsTHc3d3xzTff1PStNRhPekZjxowp838qNDRUqwyfUc2YN28eOnbsCAsLCzg6OmLQoEGIjo7WKlNdP9MOHDiA9u3bQy6Xw8fHB+Hh4TV9ezrBBKiGbdq0CdOmTcPs2bNx5swZtG3bFiEhIUhJSdF1aHqhVatWSEpK0rwOHz6s2ffuu+9ix44d2Lx5Mw4ePIjExES89NJLmv0qlQr9+/dHUVERjh49il9++QXh4eGYNWuWLm6l3svNzUXbtm2xZMmScvd/8803WLRoEZYvX44TJ07AzMwMISEhKCgo0JQZMWIELl++jL1792Lnzp04dOgQJkyYoNmvVCrRt29fNGnSBKdPn8b8+fMxZ84c/PTTTzV+fw3Bk54RAISGhmr9n9qwYYPWfj6jmnHw4EFMmjQJx48fx969e1FcXIy+ffsiNzdXU6Y6fqbFxcWhf//+6N27N86dO4d33nkH48aNw+7du2v1fmuFQDWqU6dOwqRJkzTvVSqV4OrqKsybN0+HUemH2bNnC23bti13X2ZmpmBoaChs3rxZsy0qKkoAIBw7dkwQBEH4+++/BalUKigUCk2ZZcuWCZaWlkJhYWGNxt7QARD++OMPzXu1Wi04OzsL8+fP12zLzMwU5HK5sGHDBkEQBOHKlSsCAOHkyZOaMv/8848gkUiEO3fuCIIgCEuXLhVsbGy0ns+HH34o+Pr61vAdNTwPPyNBEITRo0cLAwcOfOQxfEa1JyUlRQAgHDx4UBCE6vuZ9sEHHwitWrXSutbQoUOFkJCQmr6lWscaoBpUVFSE06dPIzg4WLNNKpUiODgYx44d02Fk+uP69etwdXWFl5cXRowYgfj4eADA6dOnUVxcrPVsmjdvjsaNG2uezbFjx9CmTRvNunQAEBISAqVSicuXL9fujTRwcXFxUCgUWs/DysoKgYGBWs/D2toaHTp00JQJDg6GVCrFiRMnNGV69OgBIyMjTZmQkBBER0fj7t27tXQ3DduBAwfg6OgIX19fTJw4Eenp6Zp9fEa1JysrCwBga2sLoPp+ph07dkzrHKVlGuJ3FhOgGpSWlgaVSqX1jw0AnJycoFAodBSV/ggMDER4eDh27dqFZcuWIS4uDt27d0d2djYUCgWMjIxgbW2tdcyDz0ahUJT77Er3UfUp/Twf939FoVDA0dFRa7+BgQFsbW35zGpJaGgo1qxZg4iICHz99dc4ePAg+vXrB5VKBYDPqLao1Wq888476Nq1K1q3bg0A1fYz7VFllEol8vPza+J2dEbnq8ET1ZR+/fpp/u7n54fAwEA0adIEv/32G0xMTHQYGVH9NGzYMM3f27RpAz8/P3h7e+PAgQPo06ePDiPTL5MmTcKlS5e0+jRS5bEGqAbZ29tDJpOV6YWfnJwMZ2dnHUWlv6ytrdGsWTPExMTA2dkZRUVFyMzM1Crz4LNxdnYu99mV7qPqU/p5Pu7/irOzc5nBAyUlJcjIyOAz0xEvLy/Y29sjJiYGAJ9RbZg8eTJ27tyJ/fv3o1GjRprt1fUz7VFlLC0tG9wvjkyAapCRkRECAgIQERGh2aZWqxEREYGgoCAdRqafcnJycOPGDbi4uCAgIACGhoZazyY6Ohrx8fGaZxMUFISLFy9q/UDfu3cvLC0t0bJly1qPvyHz9PSEs7Oz1vNQKpU4ceKE1vPIzMzE6dOnNWX27dsHtVqNwMBATZlDhw6huLhYU2bv3r3w9fWFjY1NLd2N/khISEB6ejpcXFwA8BnVJEEQMHnyZPzxxx/Yt28fPD09tfZX18+0oKAgrXOUlmmQ31m67oXd0G3cuFGQy+VCeHi4cOXKFWHChAmCtbW1Vi98qhnvvfeecODAASEuLk44cuSIEBwcLNjb2wspKSmCIAjCm2++KTRu3FjYt2+fcOrUKSEoKEgICgrSHF9SUiK0bt1a6Nu3r3Du3Dlh165dgoODgzBz5kxd3VK9lp2dLZw9e1Y4e/asAEBYsGCBcPbsWeHWrVuCIAjCV199JVhbWwvbtm0TLly4IAwcOFDw9PQU8vPzNecIDQ0V2rVrJ5w4cUI4fPiw0LRpU2H48OGa/ZmZmYKTk5MwatQo4dKlS8LGjRsFU1NT4ccff6z1+62PHveMsrOzhenTpwvHjh0T4uLihH///Vdo37690LRpU6GgoEBzDj6jmjFx4kTByspKOHDggJCUlKR55eXlacpUx8+02NhYwdTUVHj//feFqKgoYcmSJYJMJhN27dpVq/dbG5gA1YIffvhBaNy4sWBkZCR06tRJOH78uK5D0gtDhw4VXFxcBCMjI8HNzU0YOnSoEBMTo9mfn58vvPXWW4KNjY1gamoqvPjii0JSUpLWOW7evCn069dPMDExEezt7YX33ntPKC4uru1baRD2798vACjzGj16tCAI4lD4Tz75RHBychLkcrnQp08fITo6Wusc6enpwvDhwwVzc3PB0tJSGDt2rJCdna1V5vz580K3bt0EuVwuuLm5CV999VVt3WK997hnlJeXJ/Tt21dwcHAQDA0NhSZNmgjjx48v88scn1HNKO+5ABBWr16tKVNdP9P2798v+Pv7C0ZGRoKXl5fWNRoSiSAIQm3XOhERERHpEvsAERERkd5hAkRERER6hwkQERER6R0mQERERKR3mAARERGR3mECRERERHqHCRARERHpHSZARESPER4eXmaFbSKq/5gAEVG9MGbMGEgkEs3Lzs4OoaGhuHDhQoXPMWfOHPj7+9dckERUbzABIqJ6IzQ0FElJSUhKSkJERAQMDAzw/PPP6zosIqqHmAARUb0hl8vh7OwMZ2dn+Pv7Y8aMGbh9+zZSU1MBAB9++CGaNWsGU1NTeHl54ZNPPtGsOh4eHo65c+fi/Pnzmlqk8PBwAEBmZibeeOMNODk5wdjYGK1bt8bOnTu1rr179260aNEC5ubmmkSMiOovA10HQERUFTk5OVi7di18fHxgZ2cHALCwsEB4eDhcXV1x8eJFjB8/HhYWFvjggw8wdOhQXLp0Cbt27cK///4LALCysoJarUa/fv2QnZ2NtWvXwtvbG1euXIFMJtNcKy8vD99++y1+/fVXSKVSjBw5EtOnT8e6det0cu9E9PSYABFRvbFz506Ym5sDAHJzc+Hi4oKdO3dCKhUrs//3v/9pynp4eGD69OnYuHEjPvjgA5iYmMDc3BwGBgZwdnbWlNuzZw8iIyMRFRWFZs2aAQC8vLy0rltcXIzly5fD29sbADB58mR8+umnNXqvRFSzmAARUb3Ru3dvLFu2DABw9+5dLF26FP369UNkZCSaNGmCTZs2YdGiRbhx4wZycnJQUlICS0vLx57z3LlzaNSokSb5KY+pqakm+QEAFxcXpKSkVM9NEZFOsA8QEdUbZmZm8PHxgY+PDzp27Iiff/4Zubm5WLFiBY4dO4YRI0bgueeew86dO3H27Fl8/PHHKCoqeuw5TUxMnnhdQ0NDrfcSiQSCIDzVvRCRbrEGiIjqLYlEAqlUivz8fBw9ehRNmjTBxx9/rNl/69YtrfJGRkZQqVRa2/z8/JCQkIBr1649thaIiBoWJkBEVG8UFhZCoVAAEJvAFi9ejJycHAwYMABKpRLx8fHYuHEjOnbsiL/++gt//PGH1vEeHh6Ii4vTNHtZWFigZ8+e6NGjBwYPHowFCxbAx8cHV69ehUQiQWhoqC5uk4hqAZvAiKje2LVrF1xcXODi4oLAwECcPHkSmzdvRq9evfDCCy/g3XffxeTJk+Hv74+jR4/ik08+0Tp+8ODBCA0NRe/eveHg4IANGzYAAH7//Xd07NgRw4cPR8uWLfHBBx+UqSkiooZFIrAhm4iIiPQMa4CIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcMEiIiIiPQOEyAiIiLSO0yAiIiISO8wASIiIiK9wwSIiIiI9A4TICIiItI7/w+LXz5d1JMwjwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open and read the text file\n",
    "with open('models/model_stats.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Initialize empty lists for data\n",
    "epoch = []\n",
    "batch = []\n",
    "elapsed_time = []\n",
    "remaining_time = []\n",
    "train_acc = []\n",
    "\n",
    "# Extract data from each line in the file\n",
    "for line in contents.split('\\n'):\n",
    "    # Use regular expressions to extract data\n",
    "    epoch_match = re.search('Epoch: (\\d+)/\\d+', line)\n",
    "    batch_match = re.search('Batch: (\\d+)/(\\d+)', line)\n",
    "    time_match = re.search('Elapsed Time: ([\\d\\.]+)s - Remaining Time: ([\\d\\.]+)s', line)\n",
    "    acc_match = re.search('Train Acc: ([\\d\\.]+)', line)\n",
    "\n",
    "    # Append extracted data to their respective lists\n",
    "    if epoch_match:\n",
    "        epoch.append(int(epoch_match.group(1)))\n",
    "        # Append empty lists for each epoch to store batch and train accuracy data\n",
    "        batch.append([])\n",
    "        train_acc.append([])\n",
    "    if batch_match:\n",
    "        # Append batch and train accuracy data to the current epoch\n",
    "        batch[-1].append(int(batch_match.group(1)))\n",
    "    if acc_match:\n",
    "        train_acc[-1].append(float(acc_match.group(1)))\n",
    "\n",
    "# Plot the train accuracy for each epoch against batch\n",
    "for i in range(len(epoch)):\n",
    "\n",
    "    train_acc[i] = train_acc[i][:len(batch[i])]\n",
    "    plt.plot(batch[i], train_acc[i], label='Epoch {}'.format(epoch[i]))\n",
    "\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.title('Train Accuracy vs Batch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist ersichtlich, dass die Genauigkeit über die Batches in der ersten und zweiten Epoche signifikant verbessert wird. Dabei ist zu beachten, dass frühere, niedrige Genauigkeiten immer in den aktuellen Batch-Genauigkeitswert einfließen. Eine Kurve stellt somit den kumulativen Genauigkeitswert dar. Dies erklärt auch den Sprung von der ersten zur zweiten Epoche, da die “alten Lasten” des weniger trainierten Modells mit dem Epochenwechsel sozusagen entfernt wurden. Man kann sehen, dass sich das Modell ständig verbessert. Aufgrund von Hardwarebeschränkungen konnten wir dieses tiefe neuronale Netzwerk jedoch nicht weiter trainieren und verwenden daher in den folgenden Notebooks einfachere, weniger parametrisierte Modelle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training unseres Modells können wir seine Parameter in einer Datei speichern, so dass wir es später zur Bewertung oder zum weiteren Training laden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as an h5 file\n",
    "torch.save(model.state_dict(), 'models/DistilBERTSentimentAnalysis2Epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.load_state_dict(torch.load('models/DistilBERTSentimentAnalysis2Epochs.h5'))\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir unser Modell trainiert haben, können wir seine Leistung anhand eines Testdatensatzes bewerten, um ein besseres Verständnis dafür zu bekommen, wie gut es auf neue Daten verallgemeinert werden kann. Dafür müssen wir die Testdaten auf die selbe Art behandeln wie die Train und Validation Daten. Die `test_model` Funktion übernimmt alle nötigen Schritte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            input_ids = data[\"input_ids\"].squeeze(1)\n",
    "            attention_mask = data[\"attention_mask\"].squeeze(1)\n",
    "            labels = data[\"labels\"]\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=2).float() # Convert labels to one-hot encoding\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[0] # Get raw logits from model\n",
    "\n",
    "            loss = loss_fn(logits, labels_one_hot) # Compute loss using raw logits and one-hot encoded labels\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            test_acc += (pred == labels).sum().item()\n",
    "\n",
    "    # Calculate the average loss and accuracy\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_acc = test_acc / len(test_loader.dataset)\n",
    "\n",
    "    # Print the test loss and accuracy\n",
    "    print(\"Test Loss: {:.4f}, Test Acc: {:.4f}\".format(test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0248, Test Acc: 0.8239\n"
     ]
    }
   ],
   "source": [
    "test_model(model=model,test_loader=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach 2 Epochen erreicht das Modell auf den Testdaten eine Genauigkeit von etwa 82,5%, was für die Anzahl der Epochen eine gute Leistung darstellt. Es ist wahrscheinlich, dass sich das Modell bei weiterem Training über mehr Epochen noch weiter verbessern würde."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5665510ca189cb7ac4e3c864232ded5619aba5408faa44ba3e72f4f05a1b94d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
