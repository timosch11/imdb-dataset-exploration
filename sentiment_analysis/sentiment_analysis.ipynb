{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    49582\n",
       "True       418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    49582\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del punctoation\n",
    "df['review'] = df['review'].str.translate(str.maketrans(\"\", \"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the other reviewers has mentioned that after watching just 1 oz episode youll be hooked they are right as this is exactly what happened with mebr br the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordbr br it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br i would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck me as so nasty it was surreal i couldnt say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][0] #All is lowercase, <br> are removed and also punctoation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove english stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment positiv: 1, negative: 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenize the review column\n",
    "df['review'] = df['review'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Stem with the famous porter Stemmer\n",
    "df['review'] = df['review'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product br br film techniqu unass...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>thought movi right good job wasnt creativ orig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot bad dialogu bad act idiot direct anno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>cathol taught parochi elementari school nun ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>im go disagre previou comment side maltin one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>one expect star trek movi high art fan expect ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49582 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      one review mention watch 1 oz episod youll hoo...          1\n",
       "1      wonder littl product br br film techniqu unass...          1\n",
       "2      thought wonder way spend time hot summer weeke...          1\n",
       "3      basic there famili littl boy jake think there ...          0\n",
       "4      petter mattei love time money visual stun film...          1\n",
       "...                                                  ...        ...\n",
       "49995  thought movi right good job wasnt creativ orig...          1\n",
       "49996  bad plot bad dialogu bad act idiot direct anno...          0\n",
       "49997  cathol taught parochi elementari school nun ta...          0\n",
       "49998  im go disagre previou comment side maltin one ...          0\n",
       "49999  one expect star trek movi high art fan expect ...          0\n",
       "\n",
       "[49582 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(lambda x: ' '.join(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# stratified train/test split (0.7/0.3)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Get the indices of the split\n",
    "for train_index, test_index in sss.split(df.index, df['sentiment']):\n",
    "    X_train, X_test = df['review'].iloc[train_index], df['review'].iloc[test_index]\n",
    "    y_train, y_test = df['sentiment'].iloc[train_index], df['sentiment'].iloc[test_index]\n",
    "    X_test = pd.DataFrame({'review': X_test})\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test = pd.DataFrame({'sentiment': y_test})\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    # test into val and test (0.2/0.1)\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    for test_index, val_index in sss2.split(X_test.index, y_test):\n",
    "        X_test2, X_val = X_test['review'].iloc[test_index], X_test['review'].iloc[val_index]\n",
    "        y_test2, y_val = y_test['sentiment'].iloc[test_index], y_test['sentiment'].iloc[val_index]\n",
    "        \n",
    "# Save each split in a separate file\n",
    "train = pd.DataFrame({'review': X_train, 'sentiment': y_train})\n",
    "val = pd.DataFrame({'review': X_val, 'sentiment': y_val})\n",
    "test = pd.DataFrame({'review': X_test2, 'sentiment': y_test2})\n",
    "\n",
    "train.to_csv('preprocessed/train.csv', index=False)\n",
    "val.to_csv('preprocessed/val.csv', index=False)\n",
    "test.to_csv('preprocessed/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19449</th>\n",
       "      <td>work librari expect like movi came 5 year ago ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18231</th>\n",
       "      <td>eagl wing pleasant surpris movi keep viewer in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29156</th>\n",
       "      <td>new york love collect work eleven short film s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10729</th>\n",
       "      <td>saw movi yesterday night one best made tv film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27202</th>\n",
       "      <td>playwright sidney bruhl wonder overthetop mich...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31820</th>\n",
       "      <td>love movi tv program record come nov 2nd reall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35071</th>\n",
       "      <td>big jim carey fan took seat cinema optim fun d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>even 6000 buck cast parttim actor christoph no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29352</th>\n",
       "      <td>one best movi ive ever seen good act hank newm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41569</th>\n",
       "      <td>grow voyag space favorit movi rememb time ktla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34707 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "19449  work librari expect like movi came 5 year ago ...          1\n",
       "18231  eagl wing pleasant surpris movi keep viewer in...          1\n",
       "29156  new york love collect work eleven short film s...          1\n",
       "10729  saw movi yesterday night one best made tv film...          1\n",
       "27202  playwright sidney bruhl wonder overthetop mich...          1\n",
       "...                                                  ...        ...\n",
       "31820  love movi tv program record come nov 2nd reall...          1\n",
       "35071  big jim carey fan took seat cinema optim fun d...          0\n",
       "24995  even 6000 buck cast parttim actor christoph no...          1\n",
       "29352  one best movi ive ever seen good act hank newm...          1\n",
       "41569  grow voyag space favorit movi rememb time ktla...          1\n",
       "\n",
       "[34707 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('preprocessed/train.csv')\n",
    "val = pd.read_csv('preprocessed/val.csv')\n",
    "test = pd.read_csv('preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timos\\Documents\\Programming for Data Science\\sentiment_analysis\\sentiment_analysis.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     ax[i]\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mProportion of Samples\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Plot the distribution of sentiment labels in the complete dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m df_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m df_proportions \u001b[39m=\u001b[39m df_counts \u001b[39m#/ df_counts.sum()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m fig2, ax2 \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the distribution of sentiment labels in each split\n",
    "train_counts = np.bincount(train['sentiment'].astype(int))\n",
    "val_counts = np.bincount(val['sentiment'].astype(int))\n",
    "test_counts = np.bincount(test['sentiment'].astype(int))\n",
    "\n",
    "# Get the proportions of positive and negative sentiment labels\n",
    "train_proportions = train_counts #/ train_counts.sum()\n",
    "val_proportions = val_counts #/ val_counts.sum()\n",
    "test_proportions = test_counts# / test_counts.sum()\n",
    "\n",
    "# Plot the bar charts for the splits\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].bar(['positive', 'negative'], train_proportions)\n",
    "ax[1].bar(['positive', 'negative'], val_proportions)\n",
    "ax[2].bar(['positive', 'negative'], test_proportions)\n",
    "\n",
    "# Add labels and title to each chart\n",
    "ax[0].set_title('Train Split')\n",
    "ax[1].set_title('Validation Split')\n",
    "ax[2].set_title('Test Split')\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('Sentiment Label')\n",
    "    ax[i].set_ylabel('Proportion of Samples')\n",
    "\n",
    "# Plot the distribution of sentiment labels in the complete dataset\n",
    "df_counts = np.bincount(df['sentiment'].astype(int))\n",
    "df_proportions = df_counts #/ df_counts.sum()\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax2.bar(['positive', 'negative'], df_proportions)\n",
    "ax2.set_title('Complete Dataset')\n",
    "ax2.set_xlabel('Sentiment Label')\n",
    "ax2.set_ylabel('Proportion of Samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\timos\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: torch.Size([34707, 512])\n",
      "Train label shape: torch.Size([34707])\n",
      "Val input shape: torch.Size([7438, 512])\n",
      "Val label shape: torch.Size([7438])\n",
      "Test input shape: torch.Size([7437, 512])\n",
      "Test label shape: torch.Size([7437])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train input shape: {train_inputs.shape}\")\n",
    "print(f\"Train label shape: {train_labels.shape}\")\n",
    "print(f\"Val input shape: {val_inputs.shape}\")\n",
    "print(f\"Val label shape: {val_labels.shape}\")\n",
    "print(f\"Test input shape: {test_inputs.shape}\")\n",
    "print(f\"Test label shape: {test_labels.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on only experiments for sentiment analysis where done. Running sentiment analysis using Transformers can be found in sentiment_analysis2.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load DistilBERT tokenizer and tokenize (encode) the texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset\n",
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch, padding=True, truncation=True, max_length=128)\n",
    "    return tokenized_batch\n",
    "train_data = tokenize(list(train[\"review\"]))\n",
    "train_labels = list(train[\"sentiment\"])\n",
    "val_data = tokenize(list(val[\"review\"]))\n",
    "val_labels = list(val[\"sentiment\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x0000017DFFB07A60>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def to_inputs_and_labels(sample, label):\n",
    "    print(sample)\n",
    "    input_ids, attention_mask = sample[\"input_ids\"], sample[\"attention_mask\"]\n",
    "    return (torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label))\n",
    "\n",
    "train_data = map(lambda x, y: to_inputs_and_labels(x, y), train_data, train_labels)\n",
    "val_data = map(lambda x, y: to_inputs_and_labels(x, y), val_data, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate a data collator with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create data loaders for to reshape data for PyTorch model\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_data, batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Dynamically set number of class labels based on dataset\n",
    "num_labels = 2\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# Load model from checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           num_labels=num_labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Model parameters\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Further define learning rate scheduler\n",
    "num_training_batches = len(train_dataloader)\n",
    "num_training_steps = num_epochs * num_training_batches\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",                   # linear decay\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timos\\Documents\\Programming for Data Science\\sentiment_analysis\\sentiment_analysis.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> 249\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    250\u001b[0m         features,\n\u001b[0;32m    251\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    252\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m    253\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    254\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m    257\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2941\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2937\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2938\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[0;32m   2939\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2940\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 2941\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2942\u001b[0m     )\n\u001b[0;32m   2944\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[0;32m   2946\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(required_input, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(required_input) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Train the model with PyTorch training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "train_inputs, train_masks = preprocessing_for_bert(train[\"review\"])\n",
    "val_inputs, val_masks = preprocessing_for_bert(val[\"review\"])\n",
    "train_labels = torch.tensor(train[\"sentiment\"])\n",
    "val_labels = torch.tensor(val[\"sentiment\"])\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.06 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = model\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            print(b_input_ids)\n",
    "            print(b_attn_mask)\n",
    "            logits = model(input_ids=b_input_ids, attention_mask=b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        if evaluation == True:\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "tensor([[  101, 22063, 11019,  ...,  2072,  2034,   102],\n",
      "        [  101, 14689,  4747,  ..., 10024,  3089,   102],\n",
      "        [  101, 24560,  9587,  ...,  2028, 19337,   102],\n",
      "        ...,\n",
      "        [  101,  9587,  5737,  ...,  5856,  2615,   102],\n",
      "        [  101,  6415,  4115,  ...,  9587,  5737,   102],\n",
      "        [  101,  3305,  2391,  ..., 28426,  2895,   102]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timos\\Documents\\Programming for Data Science\\sentiment_analysis\\sentiment_analysis.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m set_seed(\u001b[39m42\u001b[39m)    \u001b[39m# Set seed for reproducibility\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bert_classifier, optimizer, scheduler \u001b[39m=\u001b[39m initialize_model(epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(bert_classifier, train_dataloader, val_dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\timos\\Documents\\Programming for Data Science\\sentiment_analysis\\sentiment_analysis.ipynb Cell 37\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(b_input_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(b_attn_mask)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m logits \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mb_input_ids, attention_mask\u001b[39m=\u001b[39;49mb_attn_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Compute loss and accumulate the loss values\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timos/Documents/Programming%20for%20Data%20Science/sentiment_analysis/sentiment_analysis.ipynb#X60sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, b_labels)\n",
      "File \u001b[1;32mc:\\Users\\timos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m<timed exec>:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290f75e68ed0da714247ccd610ce4a15a5bfa48e2a04a943182beeac16de2706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
